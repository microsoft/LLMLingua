"""

A module that provides a class for compressing prompts to optimize for efficient language model (LM) usage, particularly suitable for Transformer-based LMs like GPT-3.5 and similar architectures. It allows shortening inputs while preserving their semantic content to reduce token usage and computational overhead, potentially lowering the cost of using pay-per-token LMs and improving performance on token-limited models.

The main class, PromptCompressor, offers a variety of methods for processing input texts and compressing prompts using different techniques and ranking methods. It can work with multiple backends beyond local computation including BM25, sentence-BERT, and various APIs like OpenAI's embeddings and Cohere's reranker.

The module also provides a utility to reverse-engineer the compression process and recover the original uncompressed prompt from a compressed one in the context of a given response, as well as methods for ranking the relevance of context using multiple retrieval models.

Attributes:
    load_model (Callable): Loads and configures the specified Transformer-based language model for compression tasks.
    get_ppl (Callable): Calculates the perplexity of a given text using the loaded model, indicating the text's fluency.
    compress_prompt (Callable): Compresses prompts, aiming to reduce their length while maintaining the important information.
    get_token_length (Callable): Retrieves the token length of a given text as processed by the tokenization mechanic of the loaded LM.
    get_condition_ppl (Callable): Determines the perplexity of a text under a given condition, influenced by an associated query.
    control_context_budget (Callable): Filters and trims context passages to fit within a predetermined token budget.
    control_sentence_budget (Callable): Prunes individual sentences from context while respecting a total token limit.
    get_compressed_input (Callable): Executes the core compression logic, updating tokens based on calculated loss.
    iterative_compress_prompt (Callable): Iteratively compresses prompts using the class's various compression tools.
    recover (Callable): Attempts to recover the original uncompressed prompt from a compressed one based on a response.
    get_rank_results (Callable): Retrieves ranking results for context documents related to a query via different ranking methods.

Note: Documentation automatically generated by https://undoc.ai
"""
import bisect
from collections import defaultdict
from typing import List

import numpy as np
import torch

import nltk
import tiktoken
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer

encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")


class PromptCompressor:
    """

    A class for compressing prompts to use with large language models, optimizing prompt length without losing important context information.

        Attributes:
            retrieval_model: A transformer-based model for sentence embedding or ranking, initialized as None.
            retrieval_model_name: A string representing the name of the retrieval model used, initialized as None.
            open_api_config: A dictionary containing configuration for any external API used in ranking, initialized with an empty dict.
            cache_bos_num: An integer indicating the number of beginning-of-sequence tokens cached, defaulted to 10.
            prefix_bos_num: An integer indicating the number of prefix tokens before the beginning of sequence, defaulted to 100.
            tokenizer: An instance of the tokenizer used with the model.
            model: An instance of the transformer model used for causal language modeling.
            context_idxs: A list to store the indexes of context tokens.
            max_position_embeddings: An integer denoting the maximum number of position embeddings available in the model.

        The constructor of the class initializes the model and tokenization components, along with several attributes used for text processing and compression. The class offers multiple methods to assist with the compression and analysis of prompts before they are fed into a language model, facilitating the operation under token constraints and aiming to improve the efficiency of the language model. It leverages various techniques like perplexity calculation, iterative compression, and external APIs or embedding models to rank and filter the most relevant parts of the context. The class is designed with extendability for future enhancements and integrations with external retrievals systems and compression algorithms.
    """

    def __init__(
        self,
        model_name: str = "NousResearch/Llama-2-7b-hf",
        device_map: str = "cuda",
        model_config: dict = {},
        open_api_config: dict = {},
    ):
        """

        Initializes an instance with a specified language model and configurations.

            This method is responsible for loading a specified language model into the instance, initializing retrieval model parameters, and
            setting up configuration for any OpenAPI specifications. It also initializes values related to cache and prefixes
            for the model's operations.

            Args:
                model_name (str): The name or path of the pre-trained language model to load. Defaults to 'NousResearch/Llama-2-7b-hf'.
                device_map (str): The device on which to run the model, can be 'cpu' or 'cuda' for GPU acceleration. Defaults to 'cuda'.
                model_config (dict): A dictionary containing configuration parameters for the model. Defaults to an empty dict.
                open_api_config (dict): A dictionary containing configuration for OpenAPI compatibility. Defaults to an empty dict.

            Side Effects:
                Loads the specified language model into memory according to the device_map and initializes various attributes of the instance.

        """
        self.load_model(model_name, device_map, model_config)
        self.retrieval_model = None
        self.retrieval_model_name = None
        self.open_api_config = open_api_config
        self.cache_bos_num = 10
        self.prefix_bos_num = 100

    def load_model(
        self, model_name: str, device_map: str = "cuda", model_config: dict = {}
    ):
        """
        Initializes and loads a pre-trained model along with its tokenizer based on the provided model name and configuration.

        This method configures the tokenizer to account for padding preferences and sets up the model on
        the specified computing device (i.e., CPU or GPU). It can handle both simple device definitions or
        more complex mappings for distributed computing scenarios.

        Args:
            model_name (str): Name or path of the pre-trained model to be loaded.
            device_map (str): The device on which the model should be loaded. Defaults to 'cuda', but
                              can be set to 'cpu' or more complex device mappings.
            model_config (dict): A dictionary of additional configurations to apply to the model. Accepts
                                various keys such as 'pad_to_left', 'trust_remote_code', etc.,
                                which alter the model's behavior and loading process.

        The 'trust_remote_code' configuration within model_config dictates if remote code should be trusted
        when loading the tokenizer and the model components. It defaults to True if not specified. The 'pad_to_left'
        configuration determines where the tokenizer should add padding relative to the text.

        The method ensures proper setting of the pad token id based on the model's configuration or tokenizer settings
        and handles device placement of the model for torch-based models, acknowledging the presence of 'cuda' or 'cpu' in
        device_map for appropriate data type casting. It also deals with special cases for offloading and caching for
        more efficient memory usage.

        Attributes set during loading:
            self.device: Indicates the device on which the model is loaded.
            self.tokenizer: The tokenizer that is loaded and configured based on 'model_name'.
            self.model: The loaded pre-trained model, ready for use.
            self.context_idxs: (Not explicitly described in the signature, presumed to be initialized here)
            self.max_position_embeddings: Extracted from the configuration of the loaded model to limit
                                          the maximum number of position embeddings.

        Note that this function may perform network operations to download the necessary files if they are
        not already cached locally.

        Raises:
            It can raise various exceptions related to the HuggingFace Transformers library, networking issues,
            or incorrect configuration settings, which are not explicitly handled within this method.

        Returns:
            None. This method operates by side effects on the class instance.
        """
        trust_remote_code = model_config.get("trust_remote_code", True)
        if "trust_remote_code" not in model_config:
            model_config["trust_remote_code"] = trust_remote_code
        config = AutoConfig.from_pretrained(
            model_name, trust_remote_code=trust_remote_code
        )
        tokenizer = AutoTokenizer.from_pretrained(
            model_name, trust_remote_code=trust_remote_code
        )
        if model_config.get("pad_to_left", True):
            tokenizer.padding_side = "left"
            tokenizer.pad_token_id = (
                config.pad_token_id if config.pad_token_id else tokenizer.eos_token_id
            )
        self.device = (
            device_map if any(key in device_map for key in ["cuda", "cpu"]) else "cuda"
        )
        if "cuda" in device_map or "cpu" in device_map:
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype="auto" if device_map == "cuda" else torch.float32,
                device_map=device_map,
                config=config,
                ignore_mismatched_sizes=True,
                **model_config,
            )
        else:
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                device_map=device_map,
                torch_dtype="auto",
                pad_token_id=tokenizer.pad_token_id,
                offload_folder="/tmp/offload",
                offload_state_dict=True,
                cache_dir="/tmp/cache",
                **model_config,
            )
        self.tokenizer = tokenizer
        self.model = model
        self.context_idxs = []
        self.max_position_embeddings = config.max_position_embeddings

    def get_ppl(
        self,
        text: str,
        granularity: str = "sentence",
        input_ids=None,
        attention_mask=None,
        past_key_values=None,
        return_kv=False,
        end=None,
        condition_mode: str = "none",
        condition_pos_id: int = 0,
    ):
        """


        Computes the perplexity of text with respect to a language model.

        This function calculates the perplexity of a given text by utilizing a pretrained language model. Perplexity is a measurement of
        how well a probability model predicts a sample, with lower values indicating better predictive performance. It optionally
        supports partial computations by providing previously computed states. It also allows for conditioning the computation
        before or after a specified position in the sequence.

        Args:
            text (str): The text to compute perplexity for.
            granularity (str, optional): The level of granularity for the perplexity calculation, either 'sentence' or 'token'. Defaults to 'sentence'.
            input_ids (torch.Tensor, optional): Tensor containing input IDs if already computed. Defaults to None.
            attention_mask (torch.Tensor, optional): Tensor containing the attention mask if already computed. Defaults to None.
            past_key_values (tuple, optional): Tuple containing past key values if computation is to be continued from a previous state. Defaults to None.
            return_kv (bool, optional): Whether to return the past_key_values. Defaults to False.
            end (int, optional): The end position for the computation in the sequence. Defaults to None, which means it will be calculated based on input length.
            condition_mode (str, optional): Specifies if and how the computation is conditioned, either 'before', 'after', or 'none'. Defaults to 'none'.
            condition_pos_id (int, optional): The position ID for the condition if condition_mode is not 'none'. Defaults to 0.

        Returns:
            float or tuple: The perplexity of the given text if granularity is 'sentence', or a tuple containing the token-wise
                             perplexity and past_key_values if return_kv is True.

        Note:
            The `input_ids` and `attention_mask` arguments are expected to be tensors already placed on the correct device
            upon which the model should execute. States in `past_key_values` must match the configuration of the model, and
            `condition_pos_id` should be within the range of the sequence length if condition_mode is activated.

        """
        if input_ids is None:
            tokenized_text = self.tokenizer(text, return_tensors="pt")
            input_ids = tokenized_text["input_ids"].to(self.device)
            attention_mask = tokenized_text["attention_mask"].to(self.device)
        if past_key_values is not None:
            past_length = past_key_values[0][0].shape[2]
        else:
            past_length = 0
        if end is None:
            end = input_ids.shape[1]
        end = min(end, past_length + self.max_position_embeddings)
        with torch.no_grad():
            response = self.model(
                input_ids[:, past_length:end],
                attention_mask=attention_mask[:, :end],
                past_key_values=past_key_values,
                use_cache=True,
            )
            past_key_values = response.past_key_values

        loss_fct = torch.nn.CrossEntropyLoss(reduction="none")
        shift_logits = response.logits[..., :-1, :].contiguous()
        shift_labels = input_ids[..., past_length + 1 : end].contiguous()
        # Flatten the tokens
        active = (attention_mask[:, past_length:end] == 1)[..., :-1].view(-1)
        active_logits = shift_logits.view(-1, shift_logits.size(-1))[active]
        active_labels = shift_labels.view(-1)[active]
        loss_fct = torch.nn.CrossEntropyLoss(reduction="none")
        loss = loss_fct(active_logits, active_labels)
        if condition_mode == "before":
            loss = loss[:condition_pos_id]
        elif condition_mode == "after":
            loss = loss[condition_pos_id:]
        res = loss.mean() if granularity == "sentence" else loss
        return (res, past_key_values) if return_kv else res

    def __call__(self, *args, **kwargs):
        """

        Method to invoke the object as a callable which internally calls the compress_prompt method.

        This method allows the object to be used like a function. When the object is called, it defers the call
        to its internal compress_prompt method. All arguments and keyword arguments passed to
        this method are forwarded to compress_prompt.

        Args:
            *args: Variable length argument list to be passed to the compress_prompt method.
            **kwargs: Arbitrary keyword arguments to be passed to the compress_prompt method.

        Returns:
            The return value from the compress_prompt method.

        """
        return self.compress_prompt(*args, **kwargs)

    def compress_prompt(
        self,
        context: List[str],
        instruction: str = "",
        question: str = "",
        ratio: float = 0.5,
        target_token: float = -1,
        iterative_size: int = 200,
        force_context_ids: List[int] = None,
        force_context_number: int = None,
        use_sentence_level_filter: bool = False,
        use_context_level_filter: bool = True,
        use_token_level_filter: bool = True,
        keep_split: bool = False,
        keep_first_sentence: int = 0,
        keep_last_sentence: int = 0,
        keep_sentence_number: int = 0,
        high_priority_bonus: int = 100,
        context_budget: str = "+100",
        token_budget_ratio: float = 1.4,
        condition_in_question: str = "none",
        reorder_context: str = "original",
        dynamic_context_compression_ratio: float = 0.0,
        condition_compare: bool = False,
        add_instruction: bool = False,
        rank_method: str = "llmlingua",
        concate_question: bool = True,
    ):
        """


        Compresses the provided context by applying various filters and techniques to reduce token count.

        This function attempts to minimize the prompt size while preserving the most relevant information. It is part of
        a system that prepares prompts for large language models to optimize performance and cost.

        Args:
            context (List[str]): A list of context strings to be compressed.
            instruction (str, optional): Instruction to be added to the prompt. Defaults to ''.
            question (str, optional): The related question to be included in the prompt. Defaults to ''.
            ratio (float, optional): The target ratio to determine the compression level. Defaults to 0.5.
            target_token (float, optional): If not -1, explicitly sets the target token count after compression. Defaults to -1.
            iterative_size (int, optional): The batch size for iterative compression. Defaults to 200.
            force_context_ids (List[int], optional): List of context IDs to force include. Defaults to None.
            force_context_number (int, optional): Number of contexts to force include. Defaults to None.
            use_sentence_level_filter (bool, optional): Whether to apply sentence level filter. Defaults to False.
            use_context_level_filter (bool, optional): Whether to apply context level filter. Defaults to True.
            use_token_level_filter (bool, optional): Whether to apply token level filter. Defaults to True.
            keep_split (bool, optional): Whether to keep the context split. Defaults to False.
            keep_first_sentence (int, optional): Number of first sentences to keep. Defaults to 0.
            keep_last_sentence (int, optional): Number of last sentences to keep. Defaults to 0.
            keep_sentence_number (int, optional): Total number of sentences to keep. Defaults to 0.
            high_priority_bonus (int, optional): Priority bonus for certain sentences. Defaults to 100.
            context_budget (str, optional): String to determine context budget. Defaults to '+100'.
            token_budget_ratio (float, optional): Ratio for token budgeting. Defaults to 1.4.
            condition_in_question (str, optional): Conditioning to be applied to the question. Defaults to 'none'.
            reorder_context (str, optional): Method to reorder context. Defaults to 'original'.
            dynamic_context_compression_ratio (float, optional): Ratio for dynamic context compression. Defaults to 0.0.
            condition_compare (bool, optional): Whether to compare different conditions. Defaults to False.
            add_instruction (bool, optional): Whether to add instructions to the prompt. Defaults to False.
            rank_method (str, optional): Method used to rank the importance of contexts. Defaults to 'llmlingua'.
            concate_question (bool, optional): Whether to concatenate the question to the prompt. Defaults to True.

        Returns:
            dict: A dictionary with keys 'compressed_prompt' (str: the compressed prompt text), 'origin_tokens' (int: the
            original token count), 'compressed_tokens' (int: the compressed token count), 'ratio' (str: the compression
            ratio expressed as 'x.x' times), and 'saving' (str: estimated cost saving information formatted as a string).

        Raises:
            AssertionError: If 'rank_method' is 'longllmlingua' and no question is provided.


        """
        if not context:
            context = [" "]
        if isinstance(context, str):
            context = [context]
        assert not (
            rank_method == "longllmlingua" and not question
        ), "In the LongLLMLingua, it is necessary to set a question."
        if condition_compare and "_condition" not in condition_in_question:
            condition_in_question += "_condition"
        if rank_method == "longllmlingua":
            if condition_in_question == "none":
                condition_in_question = "after"
        elif rank_method == "llmlingua":
            condition_in_question = (
                "none"
                if "_condition" not in condition_in_question
                else "none_condition"
            )
        origin_tokens = len(
            encoding.encode("\n\n".join([instruction] + context + [question]).strip())
        )
        context_tokens_length = [self.get_token_length(c) for c in context]
        instruction_tokens_length, question_tokens_length = self.get_token_length(
            instruction
        ), self.get_token_length(question)
        if target_token == -1:
            target_token = (
                (
                    instruction_tokens_length
                    + question_tokens_length
                    + sum(context_tokens_length)
                )
                * (1 - ratio)
                - instruction_tokens_length
                - (question_tokens_length if concate_question else 0)
            )
        condition_flag = "_condition" in condition_in_question
        condition_in_question = condition_in_question.replace("_condition", "")

        if len(context) > 1 and use_context_level_filter:
            context, dynamic_ratio = self.control_context_budget(
                context,
                context_tokens_length,
                target_token,
                force_context_ids,
                force_context_number,
                question,
                condition_in_question,
                reorder_context=reorder_context,
                dynamic_context_compression_ratio=dynamic_context_compression_ratio,
                rank_method=rank_method,
                context_budget=context_budget,
            )
        else:
            dynamic_ratio = [0.0] * len(context)

        if use_sentence_level_filter:
            context = self.control_sentence_budget(
                context,
                target_token,
                keep_first_sentence=keep_first_sentence,
                keep_last_sentence=keep_last_sentence,
                keep_sentence_number=keep_sentence_number,
                high_priority_bonus=high_priority_bonus,
                token_budget_ratio=token_budget_ratio,
                question=question,
                condition_in_question=condition_in_question,
                rank_method=rank_method,
            )

        if condition_flag:
            prefix = question + "\n\n" + instruction if add_instruction else question
            if (
                self.get_token_length(prefix) + 2 + iterative_size * 2
                > self.max_position_embeddings
            ):
                tokens = self.tokenizer(prefix, add_special_tokens=False).input_ids
                prefix = self.tokenizer.decode(
                    tokens[: self.prefix_bos_num]
                    + tokens[
                        len(tokens)
                        - self.max_position_embeddings
                        + 2
                        + self.prefix_bos_num
                        + 2 * iterative_size :
                    ]
                )
            start = self.get_token_length(prefix) + 2
            context = [prefix] + context
        else:
            start = 0

        if use_token_level_filter:
            context = self.iterative_compress_prompt(
                context,
                target_token,
                iterative_size=iterative_size,
                keep_split=keep_split,
                start=start,
                dynamic_ratio=dynamic_ratio,
                condition_compare=condition_compare,
            )
            compressed_prompt = (
                self.tokenizer.batch_decode(context[0])[0]
                .replace("<s> ", "")
                .replace("<s>", "")
            )
        else:
            compressed_prompt = "\n\n".join(context)

        res = []
        if instruction:
            res.append(instruction)
        if compressed_prompt.strip():
            res.append(compressed_prompt)
        if question and concate_question:
            res.append(question)

        compressed_prompt = "\n\n".join(res)

        compressed_tokens = len(encoding.encode(compressed_prompt))
        saving = (origin_tokens - compressed_tokens) * 0.06 / 1000
        return {
            "compressed_prompt": compressed_prompt,
            "origin_tokens": origin_tokens,
            "compressed_tokens": compressed_tokens,
            "ratio": f"{origin_tokens / compressed_tokens:.1f}x",
            "saving": f", Saving ${saving:.1f} in GPT-4.",
        }

    def get_token_length(self, text: str, add_special_tokens: bool = True):
        """
        Calculate the length of tokens for a given text after tokenization.

        This function utilizes the tokenizer associated with the class instance to turn the input text
        into tokens. It returns the number of tokens, including or excluding special tokens based
        on the parameter provided. Special tokens are typically added to signify the beginning
        or end of sentences within the text.

        Args:
            text (str): The input text to be tokenized.
            add_special_tokens (bool): A flag indicating whether to include special tokens
                in the tokenization process. Defaults to True, meaning special tokens
                will be included in the token count.

        Returns:
            int: The total number of tokens obtained from the input text, considering
                the addition of special tokens based on the parameter.
        """
        return len(
            self.tokenizer(text, add_special_tokens=add_special_tokens).input_ids
        )

    def get_condition_ppl(
        self,
        text: str,
        question: str,
        condition_in_question: str = "none",
        granularity: str = "sentence",
    ):
        """

        Calculates the perplexity of a text given a specific condition on a question phrase. The perplexity is a measurement of how well a probability model predicts a sample. This function offers the flexibility to measure the perplexity based on different conditions relative to the question phrase, such as 'none', 'before', or 'after'. The perplexity can also be evaluated at different granularities such as on a sentence basis or other defined levels. Additionally, the function takes into account the position of the condition when concatenating the text and question phrases for evaluation.

            Args:
                text (str): The text for which perplexity should be calculated.
                question (str): The question phrase which can be used to condition the perplexity calculation.
                condition_in_question (str, optional): A string that indicates the condition relative to the question. Can be 'none', 'before', or 'after'. Defaults to 'none'.
                granularity (str, optional): A string determining the level of detail at which perplexity is calculated, e.g., 'sentence'. Defaults to 'sentence'.

            Returns:
                float: The calculated perplexity of the text under the given condition.

            Raises:
                ValueError: If an unsupported condition is specified.

            Note:
                The function internally calls 'get_ppl' and other utility methods, which are not defined within this docstring. These should be implemented appropriately to ensure proper functionality.
        """
        if condition_in_question == "none":
            return self.get_ppl(text, granularity=granularity)
        elif condition_in_question == "before":
            return self.get_ppl(
                question + text,
                granularity=granularity,
                condition_mode="after",
                condition_pos_id=self.get_token_length(question) - 1,
            )
        elif condition_in_question == "after":
            return self.get_ppl(
                text + question,
                granularity=granularity,
                condition_mode="after",
                condition_pos_id=self.get_token_length(text) - 1,
            )

    def get_dynamic_compression_ratio(
        self,
        context: list,
        target_token: float,
        iterative_size: int,
        dynamic_ratio: list,
        start: int,
    ):
        """

        Calculates dynamic compression ratios for chunks of context based on the target token, iterative size, and dynamic ratio adjustments starting at a given index. This function helps in determining the optimal compression ratio for different parts of a context to match a target token density, potentially useful for applications like text summarization or data compression where balance between detail and brevity is needed.

            Args:
                context (list): A list of textual elements for which dynamic compression ratios are calculated.
                target_token (float): The goal token density to achieve across the chunks.
                iterative_size (int): The size of each chunk for which the ratio is calculated.
                dynamic_ratio (list): A list of floating-point numbers, each corresponding to an adjustment to be applied to the token density of the respective context chunk.
                start (int): An integer indicating the starting index within the context list from where to begin processing.

            Returns:
                list: A nested list where each sublist contains tuples. Each tuple consists of the chunk size and the calculated dynamic compression ratio for that chunk. This structure represents the adjusted token densities per chunk to approximate the target token density throughout the context.
        """

        def get_ratio(base: float, delta: float):
            """


            Calculate a ratio by adding a delta to a base value, ensuring the result is clamped between 0 and 1.

            Args:
                base (float): The base value that represents the initial ratio.
                delta (float): The value to be added to the base, which can be positive or negative.

            Returns:
                float: The resulting ratio, which is guaranteed to be within the range [0, 1].

            The function takes a base floating-point value and a delta.
            It adjusts the base by adding the delta to it while making sure that the
            result does not fall below 0 or exceed 1. This is commonly used in situations
            where ratios represent things like probabilities or normalized values that are
            required to stay within this range.

            """
            return max(min(1, base + delta), 0)

        context_length = [self.get_token_length(ii, False) + 2 for ii in context]
        if start:
            context_length = context_length[1:]
        tau = target_token / (sum(context_length) + 1)
        res, idx, last, last_target = [], 0, 1, []
        while idx < len(context_length):
            if last + context_length[idx] >= iterative_size:
                last_target.append(
                    (iterative_size - last, get_ratio(tau, dynamic_ratio[idx]))
                )
                res.append(last_target)
                last = last + context_length[idx] - iterative_size
                if last > iterative_size:
                    k = last // iterative_size
                    res.extend(
                        [[(iterative_size, get_ratio(tau, dynamic_ratio[idx]))]] * k
                    )
                    last -= k * iterative_size

                last_target = (
                    [(last, get_ratio(tau, dynamic_ratio[idx]))] if last else []
                )
            else:
                last += context_length[idx]
                last_target.append(
                    (context_length[idx], get_ratio(tau, dynamic_ratio[idx]))
                )
            idx += 1
        if last_target:
            res.append(last_target)
        return res

    def control_context_budget(
        self,
        context: List[str],
        context_tokens_length: List[int],
        target_token: float,
        force_context_ids: List[int] = None,
        force_context_number: int = None,
        question: str = "",
        condition_in_question: str = "none",
        reorder_context: str = "original",
        dynamic_context_compression_ratio: float = 0.0,
        rank_method: str = "longllmlingua",
        context_budget: str = "+100",
    ):
        """


        Control context budget based on target token count and provided conditions.

        This function allows limiting and reordering the context based on a specified token count budget
        and additional rules. The selection is originally determined by an internal ranking function, but it can
        be influenced by various parameters such as forced context inclusion, reordering strategy, and dynamic
        context compression ratios. The final context is returned along with dynamic ratio adjustments if
        applicable.

        Args:
            context (List[str]): A list of context strings to consider for inclusion.
            context_tokens_length (List[int]): List of integer lengths, corresponding to the token counts
                for each context string.
            target_token (float): Initial target token count for the context budget.
            force_context_ids (List[int], optional): List of indices to forcibly include in the output
                context. Defaults to None.
            force_context_number (int, optional): Integer specifying a fixed number of contexts to include,
                overriding the token-based budget. Defaults to None.
            question (str, optional): The question string used during ranking of context. Defaults to ''.
            condition_in_question (str, optional): Condition that could affect context ranking.
                Defaults to 'none'.
            reorder_context (str, optional): Strategy to reorder the context, with possible values being
                'original', 'two_stage', or others. Defaults to 'original'.
            dynamic_context_compression_ratio (float, optional): Specifies the compression ratio for dynamic
                context adjustment. Defaults to 0.0.
            rank_method (str, optional): The ranking method name to be used by the internal ranking function.
                Defaults to 'longllmlingua'.
            context_budget (str, optional): A string representing an adjustment operation (++/-) for the final
                context budget based on the initial target_token. Defaults to '+100'.

        Returns:
            tuple:
                res (List[str]): The final list of context strings after applying budget and reordering rules.
                dynamic_ratio (List[float]): List of dynamic ratio adjustments for each context string,
                    applicable if dynamic_context_compression_ratio is greater than 0.
        """
        if force_context_ids is not None:
            return [context[ii] for ii in force_context_ids]
        demostrations_sort = self.get_rank_results(
            context,
            question,
            rank_method,
            condition_in_question,
            context_tokens_length,
        )

        if target_token < 0:
            target_token = 100
        target_token = eval("target_token" + context_budget)
        res = []
        used = force_context_ids if force_context_ids is not None else []

        self.context_idxs.append([x for idx, (x, _) in enumerate(demostrations_sort)])
        for idx, _ in demostrations_sort:
            if idx >= len(context_tokens_length):
                continue
            target_token -= context_tokens_length[idx]
            if idx not in used:
                used.append(idx)
            if target_token < 0 or (
                force_context_number is not None and len(res) >= force_context_number
            ):
                break
        original_used = used
        if reorder_context == "original":
            used = sorted(used)
        elif reorder_context == "two_stage":
            l, r = [_ for idx, _ in enumerate(used) if idx % 2 == 0], [
                _ for idx, _ in enumerate(used) if idx % 2 == 1
            ]
            used = l + r[::-1]

        if dynamic_context_compression_ratio > 0:
            N = len(used)
            if condition_in_question:
                rank = [
                    i
                    for i, _ in self.get_rank_results(
                        context,
                        question,
                        "longllmlingua",
                        "after",
                        context_tokens_length,
                    )
                ]
                used = sorted(used, key=lambda x: rank.index(x))
            dynamic_ratio = [
                i * (abs(dynamic_context_compression_ratio) / (N - 1)) if N > 1 else 0
                for i in range(-(N - 1), N, 2)
            ][::-1]
            dynamic_ratio_map = {i: j for i, j in zip(original_used, dynamic_ratio)}
            dynamic_ratio = [dynamic_ratio_map[i] for i in used]
        else:
            dynamic_ratio = [0.0] * len(used)

        res = [context[idx] for idx in used if idx < len(context)]
        return res, dynamic_ratio

    def control_sentence_budget(
        self,
        context: List[str],
        target_token: float,
        keep_first_sentence: int = 0,
        keep_last_sentence: int = 0,
        keep_sentence_number: int = 0,
        high_priority_bonus: int = 100,
        token_budget_ratio: float = 1.4,
        question: str = "",
        condition_in_question: str = "none",
        rank_method: str = "longllmlingua",
    ):
        """


        Control the distribution of sentences in the given context to fit a token budget while maintaining important information.

        This function processes the provided context to distribute sentence prominence based on a variety of criteria,
        such as whether to keep the first and/or last sentence, a fixed number of sentences to keep, and a measure
        of sentence importance (sentence_ppl). It allows for the optional prioritization of sentences based on
        a bonus for high priority sentences and ranks sentences according to a specified method. The resulting
        selection of sentences aims to retain essential content within a token budget, scaled by a specified ratio.

        Args:
            context (List[str]): The list of strings representing the context to be processed.
            target_token (float): The target number of tokens to aim for within the token budget.
            keep_first_sentence (int, optional): The number of initial sentences to keep. Defaults to 0.
            keep_last_sentence (int, optional): The number of final sentences to keep. Defaults to 0.
            keep_sentence_number (int, optional): The fixed number of sentences to keep from each section. Defaults to 0.
            high_priority_bonus (int, optional): The bonus score added to high priority sentences. Defaults to 100.
            token_budget_ratio (float, optional): The scaling factor applied to the token budget. Defaults to 1.4.
            question (str, optional): The question or query related to the context. Defaults to an empty string.
            condition_in_question (str, optional): The condition that affects sorting of sentence prominence. Defaults to 'none'.
            rank_method (str, optional): The method used for ranking sentences. Defaults to 'longllmlingua'.

        Returns:
            List[str]: A list of strings representing the processed context with sentences selected to fit the token budget.


        """

        def keep_sentence(dem_idx: int, sent_keep: int):
            """


            Selects a subset of sentences with the lowest perplexity scores and increases their priority.

            This function sorts sentences based on their perplexity scores, which are a measure of how well
            the language model predicts the sentence. It selects the top 'sent_keep' sentences with the
            lowest perplexity scores. It then increases the priority of these selected sentences by adding
            a 'high_priority_bonus' to their perplexity scores, potentially altering their order for future
            processing steps.

            Args:
                dem_idx (int): The index of the demographic group for which sentences are being evaluated.
                sent_keep (int): The number of sentences to keep based on their perplexity scores.

            Side Effects:
                Modifies the perplexity scores ('sentence_ppl') of selected sentences, by increasing them
                using the global variable 'high_priority_bonus'. The 'sentence_ppl' and 'high_priority_bonus'
                variables must be accessible within the scope of this function.

            Note:
                The calling script must have the variables 'dem_g' (a list of lists containing sentence indices for
                different demographic groups) and 'sentence_ppl' (a list of perplexity scores for individual sentences)
                defined in the global scope. The function assumes that 'dem_g' and 'sentence_ppl' have been
                populated appropriately before the function call.
            """
            idxs = sorted(dem_g[dem_idx], key=lambda x: sentence_ppl[x])[:sent_keep]
            for idx in idxs:
                sentence_ppl[idx] += high_priority_bonus

        sentences = [nltk.sent_tokenize(c) for c in context]
        dem_g, s2de, idx = defaultdict(set), defaultdict(int), 0
        for idx_d, s in enumerate(sentences):
            for _ in s:
                dem_g[idx_d].add(idx)
                s2de[idx] = idx_d
                idx += 1

        context_sentences = [s for ii in sentences for s in ii]
        sentence_tokens_length = [
            self.get_token_length(sentence) for sentence in context_sentences
        ]
        N = len(context_sentences)
        flags = list(range(len(context_sentences)))
        if len(sentence_tokens_length) == 1:
            return context
        if rank_method == "longllmlingua":
            sentence_ppl = [
                self.get_condition_ppl(sentence, question, condition_in_question)
                .cpu()
                .numpy()
                .item()
                for sentence in context_sentences
            ]
            if keep_first_sentence:
                sentence_ppl[:keep_first_sentence] = [
                    ii + high_priority_bonus
                    for ii in sentence_ppl[:keep_first_sentence]
                ]
            if keep_last_sentence:
                sentence_ppl[-keep_last_sentence:] = [
                    ii + high_priority_bonus
                    for ii in sentence_ppl[-keep_last_sentence:]
                ]
            if keep_sentence_number:
                for dem_idx in range(len(sentences)):
                    keep_sentence(dem_idx, keep_sentence_number)
            sort_direct = -1 if condition_in_question == "none" else 1
            sent_sort = sorted(
                enumerate(sentence_ppl), key=lambda x: sort_direct * x[1]
            )
        else:
            sent_sort = self.get_rank_results(
                context_sentences,
                question,
                rank_method,
                condition_in_question,
                [0] * len(context_sentences),
            )

        sentence_flags = [False] * N
        if target_token < 0:
            target_token = 100
        target_token *= token_budget_ratio
        res = []
        for idx, _ in sent_sort:
            idx = flags[idx]
            target_token -= sentence_tokens_length[idx]
            sentence_flags[idx] = True
            if target_token < 0:
                break
        idx = 0
        res = []
        for s in sentences:
            tmp = [jj for ii, jj in enumerate(s) if sentence_flags[idx + ii]]
            res.append("\n".join(tmp))
            idx += len(s)
        return res

    def get_compressed_input(
        self,
        loss,
        input_ids,
        attention_mask,
        end=200,
        iterative_size=200,
        threshold=0.5,
        keep_flag=None,
        split_token_id: int = 13,
        start: int = 0,
        self_loss=None,
        self_input_ids=None,
        self_attention_mask=None,
    ):
        """

        Computes a compressed version of the input by selectively keeping tokens based on their corresponding loss values and other provided criteria.

        Args:
            loss (Tensor): A 1D tensor containing the loss values associated with each token.
            input_ids (Tensor): The input tensor containing token IDs for the model's input sequence.
            attention_mask (Tensor): A 1D tensor that indicates which tokens should be attended to.
            end (int, optional): Position up to which tokens should be considered for compression. Defaults to 200.
            iterative_size (int, optional): Number of tokens to factor in when performing iterative compression. Defaults to 200.
            threshold (float, optional): Loss threshold to determine which tokens to keep during compression. Defaults to 0.5.
            keep_flag (Tensor, optional): A binary tensor indicating tokens that must be kept regardless of the loss value. Defaults to None.
            split_token_id (int, optional): The token ID used to identify split points in the input sequence. Defaults to 13.
            start (int, optional): The starting index for evaluating loss in case of a self-comparison. Defaults to 0.
            self_loss (Tensor, optional): A tensor of loss values for self-comparison. Defaults to None.
            self_input_ids (Tensor, optional): The tensor containing token IDs for self-comparison. Defaults to None.
            self_attention_mask (Tensor, optional): The tensor for attention mask in self-comparison. Defaults to None.

        Returns:
            Tuple[Tensor, Tensor, Tensor, int, Tensor, Tensor, Tensor, Tensor]: A tuple containing:
                - compressed_input_ids: The compressed sequence of input token IDs.
                - compressed_attention_mask: The compressed attention mask corresponding to compressed_input_ids.
                - keep_flag: Adjusted binary tensor indicating which tokens should be kept post-compression.
                - end: An integer representing the updated end index after compression.
                - loss: A tensor containing the updated loss values after compression.
                - self_loss: The updated self_loss tensor after applying the compression criteria.
                - self_compressed_input_ids: The compressed input IDs from self-comparison.
                - self_compressed_attention_mask: The compressed attention mask from self-comparison.

        Raises:
            ValueError: If any of the tensor shapes are not compatible with the operation.
        """
        if self_loss is not None:
            need_idx = torch.concat(
                [
                    loss[:start] > 0,
                    self_loss[: loss[start:].shape[0]] - loss[start:] > threshold,
                    loss[:1] > 0,
                ]
            )
        else:
            need_idx = torch.concat([loss > threshold, loss[:1] > 0])
        need_idx[end:] = 1
        need_idx[: end - iterative_size] = 1
        loss = loss[need_idx[:-1]]
        if self_loss is not None:
            if need_idx.shape[0] < self_loss.shape[0] + start + 1:
                need_idx = torch.cat(
                    [
                        need_idx,
                        torch.ones(
                            self_loss.shape[0] - need_idx.shape[0] + start + 1,
                            dtype=torch.bool,
                        ).to(need_idx.device),
                    ]
                )
            self_loss = self_loss[need_idx[start:-1]]

        if need_idx.shape[0] < input_ids.shape[1]:
            need_idx = torch.cat(
                [
                    need_idx,
                    torch.ones(
                        input_ids.shape[1] - need_idx.shape[0], dtype=torch.bool
                    ).to(need_idx.device),
                ]
            )
        elif need_idx.shape[0] > input_ids.shape[1]:
            need_idx = need_idx[: input_ids.shape[1]]

        if keep_flag is not None:
            need_idx[keep_flag == 1] = 1
        last = -1
        if keep_flag is not None:
            for ii in range(end - iterative_size, end):
                if need_idx[ii] != 1:
                    continue
                now = input_ids[0][ii].detach().cpu().item()
                if (
                    now == split_token_id
                    and last == split_token_id
                    and keep_flag[ii].detach().cpu().item() == 0
                ):
                    need_idx[ii] = 0
                else:
                    last = now
        compressed_input_ids = input_ids[attention_mask == 1][need_idx].unsqueeze(0)
        compressed_attention_mask = attention_mask[attention_mask == 1][
            need_idx
        ].unsqueeze(0)

        if self_loss is not None:
            self_compressed_input_ids = self_input_ids[self_attention_mask == 1][
                need_idx[start:]
            ].unsqueeze(0)
            self_compressed_attention_mask = self_attention_mask[
                self_attention_mask == 1
            ][need_idx[start:]].unsqueeze(0)
        else:
            self_compressed_input_ids, self_compressed_attention_mask = None, None
        if keep_flag is not None:
            if len(keep_flag) > len(need_idx):
                keep_flag = torch.cat(
                    [
                        keep_flag[:start],
                        keep_flag[start : len(need_idx) + start][need_idx],
                        keep_flag[start + len(need_idx) :],
                    ]
                )
            else:
                keep_flag = keep_flag[need_idx]
        end -= (need_idx[:end] == 0).sum()
        return (
            compressed_input_ids,
            compressed_attention_mask,
            keep_flag,
            end,
            loss,
            self_loss,
            self_compressed_input_ids,
            self_compressed_attention_mask,
        )

    def get_estimate_threshold_base_distribution(
        self, ppl, ratio: float, condition_flag: bool = False
    ):
        """

        Calculate the threshold value for a list of perplexity (ppl) scores that corresponds to a certain ratio of the distribution.

            The function filters out the maximum possible perplexity score, sorts the remaining scores based on the condition_flag,
            and then selects the threshold perplexity score corresponding to the given ratio.

            Parameters:
                ppl (torch.Tensor): A tensor containing perplexity scores, must not have any score equal to 10000 (default filtering value).
                ratio (float): A value between 0 and 1 that represents the ratio of the distribution to consider for the threshold calculation.
                condition_flag (bool, optional): A flag to determine the sorting order of the perplexity scores. If True, sorts in ascending order.
                    If False, sorts in descending order. Default is False.

            Returns:
                float: The perplexity value at the given ratio of the sorted list.

            Raises:
                ValueError: If the ratio provided is outside the range [0, 1].
        """
        ppl = ppl[ppl != 10000]
        target_token = max(0, min(len(ppl) - 1, int(len(ppl) * ratio) - 1))
        return (
            ppl.sort(descending=not condition_flag)
            .values[target_token]
            .detach()
            .cpu()
            .item()
        )

    def iterative_compress_prompt(
        self,
        context: List[str],
        target_token: float,
        iterative_size: int = 200,
        keep_split: bool = False,
        split_token_id: int = 13,
        start: int = 0,
        dynamic_ratio: list = None,
        condition_compare: bool = False,
    ):
        """


        Performs an iterative text compression on a given context to fit within a target token budget, possibly maintaining certain characteristics.

        This function is part of a model that uses token-level perplexity and text compression to conditionally compress the input context.
        It aims to achieve a target number of tokens ('target_token') through an iterative process, allowing for
        different compression ratios and optionally preserving sentence demarcation when specified.

        Args:
            context (List[str]): A list of strings, each representing a text segment of the context to be compressed.
            target_token (float): The desired token count to achieve after compression.
            iterative_size (int, optional): The size of text to compress in each iteration. Defaults to 200.
            keep_split (bool, optional): Specifies whether to preserve tokens that indicate sentence boundaries.
                Defaults to False.
            split_token_id (int, optional): The token ID used to identify sentence split points. Defaults to 13.
            start (int, optional): The starting index from which to begin compression. Defaults to 0.
            dynamic_ratio (list, optional): List of ratios for dynamic adjustment of compression during iteration.
                Defaults to None.
            condition_compare (bool, optional): Indicates whether to perform conditional comparison to maintain
                the integrity of information relative to another set of data during compression. Defaults to False.

        Returns:
            tuple: A tuple containing the compressed input IDs and attention mask as tensors, clipped starting
                from the 'start' index to the end of the available sequence.

        Note:
            'target_token', 'iterative_size', and 'dynamic_ratio' are used to determine the compression ratios
            over iterations. This function assumes access to other methods like 'get_dynamic_compression_ratio',
            'tokenizer', 'get_ppl', and 'get_compressed_input', which should be part of the class implementation.

            The returned tensors 'compressed_input_ids' and 'compressed_attention_mask' represent the compressed
            version of the input context that is amenable to downstream processing by a model expecting tokenized input.

            This function is designed to work within the constraints imposed by transformer models, particularly
            the maximum position embeddings limit of the model.

            The use of 'device' attribute, 'max_position_embeddings', and 'cache_bos_num' suggests this function
            is part of a class that handles tokenization and compression with respect to deep learning model constraints.


        """
        iterative_ratios = self.get_dynamic_compression_ratio(
            context, target_token, iterative_size, dynamic_ratio, start
        )
        context = "\n\n".join(context)
        tokenized_text = self.tokenizer(context, return_tensors="pt")
        input_ids = tokenized_text["input_ids"].to(self.device)
        attention_mask = tokenized_text["attention_mask"].to(self.device)

        N = (attention_mask == 1).sum()
        compressed_input_ids, compressed_attention_mask = input_ids, attention_mask
        if condition_compare:
            self_input_ids, self_attention_mask = (
                input_ids[:, start:],
                attention_mask[:, start:],
            )
            self_compressed_input_ids, self_compressed_attention_mask = (
                self_input_ids,
                self_attention_mask,
            )

        end = min(iterative_size + start, compressed_input_ids.shape[1])
        threshold, keep_flag = None, None
        if keep_split:
            input_ids_numpy = input_ids.cpu().detach().numpy()[0]
            N = len(input_ids_numpy)
            keep_flag = [
                int(
                    (
                        ii > 0
                        and input_ids_numpy[ii] == split_token_id
                        and input_ids_numpy[ii - 1] == split_token_id
                    )
                    or (
                        ii < N - 1
                        and input_ids_numpy[ii] == split_token_id
                        and input_ids_numpy[ii + 1] == split_token_id
                    )
                )
                for ii in range(N)
            ]
            keep_flag = torch.tensor(keep_flag).to(self.device)
        past_key_values, past_loss, ready_end = None, None, 0
        self_past_key_values, self_past_loss, self_ready_end = None, None, 0
        pop_compressed_input_ids, pop_self_compressed_input_ids = None, None
        idx = 0
        while end <= compressed_input_ids.shape[1]:
            if end > self.max_position_embeddings and past_key_values is not None:
                # KV-Cache Compression
                e, s = end - self.max_position_embeddings, self.cache_bos_num
                if pop_compressed_input_ids is None:
                    pop_compressed_input_ids = compressed_input_ids[:, :e]
                else:
                    pop_compressed_input_ids = torch.cat(
                        [pop_compressed_input_ids, compressed_input_ids[:, :e]], dim=-1
                    )
                compressed_input_ids = compressed_input_ids[:, e:]
                compressed_attention_mask = compressed_attention_mask[:, e:]
                past_key_values = [
                    [
                        torch.cat([k[..., :s, :], k[..., s + e :, :]], dim=-2),
                        torch.cat([v[..., :s, :], v[..., s + e :, :]], dim=-2),
                    ]
                    for k, v in past_key_values
                ]
                end, ready_end = end - e, ready_end - e
                if condition_compare:
                    s = min(s, self_past_key_values[0][0].shape[2] - e)
                    self_ready_end -= e
                    if pop_self_compressed_input_ids is None:
                        pop_self_compressed_input_ids = self_compressed_input_ids[:, :e]
                    else:
                        pop_self_compressed_input_ids = torch.cat(
                            [
                                pop_self_compressed_input_ids,
                                self_compressed_input_ids[:, :e],
                            ],
                            dim=-1,
                        )
                    self_compressed_input_ids = self_compressed_input_ids[:, e:]
                    self_compressed_attention_mask = self_compressed_attention_mask[
                        :, e:
                    ]
                    self_past_key_values = [
                        [
                            torch.cat([k[..., :s, :], k[..., s + e :, :]], dim=-2),
                            torch.cat([v[..., :s, :], v[..., s + e :, :]], dim=-2),
                        ]
                        for k, v in self_past_key_values
                    ]

            loss, past_key_values = self.get_ppl(
                "",
                "token",
                compressed_input_ids,
                compressed_attention_mask,
                past_key_values=past_key_values,
                return_kv=True,
                end=end if idx else None,
            )
            if past_loss is not None:
                if end - 1 > len(past_loss):
                    past_loss = torch.cat(
                        [past_loss, torch.zeros_like(loss)[: end - 1 - len(past_loss)]]
                    )
                past_loss[ready_end : end - 1] = loss
                loss = past_loss
            else:
                past_loss = loss
            if idx:
                past_key_values = [
                    [k[:, :, : end - iterative_size], v[:, :, : end - iterative_size]]
                    for k, v in past_key_values
                ]
            else:
                past_key_values = None

            if condition_compare:
                self_loss, self_past_key_values = self.get_ppl(
                    "",
                    "token",
                    self_compressed_input_ids,
                    self_compressed_attention_mask,
                    past_key_values=self_past_key_values,
                    return_kv=True,
                    end=end - start if idx else None,
                )
                if self_past_loss is not None:
                    if end - start - 1 > len(self_past_loss):
                        self_past_loss = torch.cat(
                            [
                                self_past_loss,
                                torch.zeros_like(self_loss)[
                                    : end - 1 - start - len(self_past_loss)
                                ],
                            ]
                        )
                    self_past_loss[self_ready_end : end - start - 1] = self_loss
                    self_loss = self_past_loss
                else:
                    self_past_loss = self_loss
                if idx:
                    self_past_key_values = [
                        [
                            k[:, :, : end - iterative_size - start],
                            v[:, :, : end - iterative_size - start],
                        ]
                        for k, v in self_past_key_values
                    ]
                else:
                    self_past_key_values = None

                self_ready_end = (
                    end - start - iterative_size if not (start and idx == 0) else 0
                )
            ready_end = end - iterative_size if not (start and idx == 0) else 0

            for delta_end, ratio in iterative_ratios[idx]:
                loss = past_loss
                if condition_compare:
                    self_loss = self_past_loss
                    threshold = self.get_estimate_threshold_base_distribution(
                        self_loss[: loss[start:].shape[0]] - loss[start:], ratio, False
                    )
                else:
                    threshold = self.get_estimate_threshold_base_distribution(
                        loss, ratio, False
                    )

                (
                    compressed_input_ids,
                    compressed_attention_mask,
                    keep_flag,
                    end,
                    past_loss,
                    self_past_loss,
                    self_compressed_input_ids,
                    self_compressed_attention_mask,
                ) = self.get_compressed_input(
                    loss,
                    compressed_input_ids,
                    compressed_attention_mask,
                    end - iterative_size + delta_end,
                    iterative_size=delta_end,
                    threshold=threshold,
                    keep_flag=keep_flag,
                    split_token_id=split_token_id,
                    start=start,
                    self_loss=self_loss if condition_compare else None,
                    self_input_ids=self_compressed_input_ids
                    if condition_compare
                    else None,
                    self_attention_mask=self_compressed_attention_mask
                    if condition_compare
                    else None,
                )
                end += iterative_size
            idx += 1
        if pop_compressed_input_ids is not None:
            compressed_input_ids = torch.cat(
                [pop_compressed_input_ids, compressed_input_ids], dim=-1
            )
        return compressed_input_ids[:, start:], compressed_attention_mask[:, start:]

    def recover(
        self,
        original_prompt: str,
        compressed_prompt: str,
        response: str,
    ):
        """

        Calculates a recovered version of a response using the original and compressed prompts.

        This function attempts to reconstruct the response based on the mapping from the
        original uncompressed prompt to a compressed version and a response generated
        from the compressed prompt. It uses the `match_from_compressed` inner function to
        transform words found in the compressed prompt back into their original form
        found in the original prompt. Words that are not found are included as they are.

        Args:
            original_prompt (str): The original uncompressed prompt.
            compressed_prompt (str): The compressed form of the original prompt.
            response (str): The response generated from the compressed prompt which may
                            contain compressed tokens or phrases that need to be
                            expanded back to their original form.

        Returns:
            str: The recovered response with tokens and phrases expanded to their
                 original form as per the original prompt.

        Raises:
            This function does not explicitly raise any exceptions, but might raise
            exceptions indirectly if there are issues during processing, such as
            if the tokenization process fails or if there are mismatches in data structures
            used for reconstructing the original text from the compressed version.
        """

        def match_from_compressed(response_word):
            """

            Determines the best matching sequence of words inside a compressed text block that matches the given response word.

            This function searches for the sequence that has the highest overlap with the response word within a certain window size.
            It utilizes a tokenizer to convert the response words into input IDs, and performs a search within a pre-defined corpus
            defined by `original_input_ids` which presumably contains tokenized input IDs of a larger text.

            Args:
                response_word (str): The word or sequence of words to be matched within the compressed text.

            Returns:
                str: The best matching sequence of words from the original corpus. If no match is found, returns the original
                     response word.

            Note:
                This docstring assumes the presence of instance attributes and possibly other methods such as `self.tokenizer`
                which are not detailed within this function. The original_input_ids and the length of the corpus (M) are also assumed
                to be attributes or otherwise accessible variables within the context of this function's execution.



            """
            response_input_ids = self.tokenizer(
                response_word, add_special_tokens=False
            )["input_ids"]
            response_set, response_c = set(response_input_ids), defaultdict(list)
            for idx in range(M):
                if original_input_ids[idx] in response_set:
                    response_c[original_input_ids[idx]].append(idx)
            res, res_min, res_c = None, float("inf"), 1
            n = len(response_input_ids)
            for l in response_c[response_input_ids[0]]:
                x, y, c = 0, l, 1
                for x in range(1, n):
                    idx = bisect.bisect_right(response_c[response_input_ids[x]], y)
                    if (
                        idx >= len(response_c[response_input_ids[x]])
                        or response_c[response_input_ids[x]][idx] - y > 10
                    ):
                        continue
                    c += 1
                    y = response_c[response_input_ids[x]][idx]
                if c > res_c:
                    res_c = c
                    res_min = y - l + 1
                    res = (l, y + 1)
                elif c == res_c and y - l + 1 < res_min:
                    res_min = y - l + 1
                    res = (l, y + 1)

            if res is None:
                return response_word
            # while l > 0 and not self.tokenizer.convert_ids_to_tokens(original_input_ids[l]).startswith("_"):
            #     l -= 1
            # while r < M - 1 and not self.tokenizer.convert_ids_to_tokens(original_input_ids[l]).startswith("_"):
            #     l -= 1
            return self.tokenizer.decode(original_input_ids[res[0] : res[1]])

        response_words = response.split(" ")

        original_input_ids = self.tokenizer(original_prompt, add_special_tokens=False)[
            "input_ids"
        ]
        N, M = len(response_words), len(original_input_ids)
        recovered_response_words = []
        l = 0
        while l < N:
            if response_words[l] not in compressed_prompt:
                recovered_response_words.append(response_words[l])
                l += 1
                continue
            r = l
            while (
                r + 1 < N and " ".join(response_words[l : r + 2]) in compressed_prompt
            ):
                r += 1

            match_words = match_from_compressed(" ".join(response_words[l : r + 1]))
            recovered_response_words.append(match_words)
            l = r + 1
        return " ".join(recovered_response_words)

    def get_rank_results(
        self,
        context: list,
        question: str,
        rank_method: str,
        condition_in_question: str,
        context_tokens_length: list,
    ):
        """

        Calculates the ranking of documents in a given corpus based on a query and specified ranking method.

            This function serves as a wrapper to call different ranking algorithms depending on the rank method provided.
            It supports various state-of-the-art algorithms and model-based ranking methods by utilizing their respective
            libraries and APIs. The ranking is mostly based on the relevance of documents in the corpus to the given query.

            Args:
                context (list): A list of strings, which constitute the corpus of documents to rank.
                question (str): The query string based on which the ranking of the documents is to be determined.
                rank_method (str): A string identifier to select the ranking algorithm or model. Supported methods include
                                  'bm25', 'gzip', 'sentbert', 'openai', 'longllmlingua', 'llmlingua', 'bge',
                                  'bge_reranker', 'bge_llmembedder', 'jinza', 'voyageai', 'cohere'.
                condition_in_question (str): An additional parameter for certain ranking methods that influences how the
                                             ranking is calculated. Specific usage varies based on the ranking method.
                context_tokens_length (list): A list of integers representing the token length of each document in the
                                              context. This is used in specific ranking methods that account for
                                              document length.

            Returns:
                list: A list of tuples, where each tuple contains an index of the document in the original context list
                      and a zero placeholder score, sorted according to their relevance to the query as determined by
                      the specified rank method.

            Raises:
                ValueError: If an unsupported rank method is specified.

            Note:
                Each internal ranking function requires specific third-party libraries, models, or API access and
                may need additional configuration or setup outside the context of this function.
                The zero placeholder score in the returned tuples does not represent actual scores and is intended
                for compatibility with expected return formats that may include score values.
        """

        def get_distance_bm25(corpus, query):
            """

            Calculates and ranks documents in a corpus based on their BM25 score relative to a given query.

            The function first tokenizes both the corpus and the query, splitting them into lists of words. It then uses the BM25Okapi model from the `rank_bm25` library to calculate the relevance of each document in the corpus to the query. The BM25 score for each document with respect to the query is computed, and the documents are sorted in descending order of their scores, implying that the most relevant document is ranked first.

            Args:
                corpus (List[str]): The list of documents, each document as a string.
                query (str): The search query as a string.

            Returns:
                List[Tuple[int, int]]: A list of tuples, each containing the index of the document in the corpus and a static integer `0`, sorted by decreasing BM25 score.
            """
            from rank_bm25 import BM25Okapi

            tokenized_corpus = [doc.split(" ") for doc in corpus]
            bm25 = BM25Okapi(tokenized_corpus)
            tokenized_query = query.split(" ")
            doc_scores = bm25.get_scores(tokenized_query)
            idx = [(ii, 0) for ii in (-doc_scores).argsort()]
            return idx

        def get_distance_gzip(corpus, query):
            """


                Calculate and rank documents in the corpus based on their gzip-compressed distance to a query.

                This function computes the distance of each document in the corpus from the provided query using
                gzip compression. The distance score is calculated using a custom get_score function, which
            takes two strings, compresses them separately and together, and measures the difference.
                The distances are then used to rank the documents in ascending order of their distance score.

                Parameters
                ----------
                corpus : list
                    A list of strings, where each string represents a document.
                query : str
                    A query string whose distance to each document in the corpus is to be calculated.

                Returns
                -------
                list
                    A list of tuples, each containing an index corresponding to a document and a fixed value of 0.
                    The list is sorted by the ascending distance of the documents from the query.


            """

            def get_score(x, y):
                """

                Calculates a normalized compression-based similarity score between two strings.

                The function computes the similarity score by using gzip compression to assess how
                much the compressed version of the concatenation of the two strings saves in
                space in comparison to the compressed versions of the individual strings. The
                score is given by the difference in the lengths of the compressed concatenated
                string and the smaller of the compressed individual strings, normalized by the
                length of the larger of the compressed individual strings.

                Args:
                    x (str): The first string to be compared.
                    y (str): The second string to be compared.

                Returns:
                    float: A score that represents the normalized compression-based similarity
                    between the two input strings. A lower score indicates higher similarity.

                Note:
                    This function requires the 'gzip' module to be imported prior to its use.

                """
                cx, cy = len(gzip.compress(x.encode())), len(gzip.compress(y.encode()))
                cxy = len(gzip.compress(f"{x} {y}".encode()))
                return (cxy - min(cx, cy)) / max(cx, cy)

            import gzip

            doc_scores = [get_score(doc, query) for doc in corpus]
            idx = [(ii, 0) for ii in np.argsort(doc_scores)]
            return idx

        def get_distance_sentbert(corpus, query):
            """


            Compute the semantic similarity between corpus documents and a query string using Sentence-BERT.

            This function leverages the SentenceTransformer model to encode documents and a query into embeddings and
            computes the cosine similarity score. It returns indexes of the documents in the corpus sorted by
            similarity score.

            Parameters:
                corpus (list of str): A list of documents represented as strings.
                query (str): The query string whose similarity to the corpus is to be computed.

            Returns:
                list of tuple: A list of tuples, each containing the index of the document in the original
                corpus list and a placeholder 0 value (for compatibility with certain applications),
                sorted by the similarity score in descending order.

            Note:
                The function assumes that the retrieval_model attribute is a SentenceTransformer instance.
                It will instantiate a new model and assign it to retrieval_model if it doesn't exist or if the
                retrieval_model_name does not match the rank_method variable. The rank_method is not
                defined within the function and should be globally defined or passed as a parameter.

                The function imports the necessary SentenceTransformer and util modules from the sentence_transformers package.

                The placeholder '0' in the returned tuples does not represent an actual score and may pertain to
                additional context or functionality not described here.

            Raises:
                ImportError: If the required modules are not available.


            """
            from sentence_transformers import SentenceTransformer, util

            if self.retrieval_model is None or self.retrieval_model_name != rank_method:
                self.retrieval_model = SentenceTransformer("multi-qa-mpnet-base-dot-v1")
                self.retrieval_model_name = rank_method
            doc_embeds = self.retrieval_model.encode(corpus)
            query = self.retrieval_model.encode(query)
            doc_scores = -util.dot_score(doc_embeds, query).cpu().numpy().reshape(-1)
            idx = [(ii, 0) for ii in np.argsort(doc_scores)]
            return idx

        def get_distance_openai(corpus, query):
            """

            Calculates semantic distance between a given query and a set of documents using OpenAI's text-embedding model and returns sorted indices with scores indicating the closeness of each document to the query in descending order of relevance.

                This function utilizes OpenAI's API to generate embeddings for the input query and a corpus of documents. It uses the dot product of the embeddings as a semantic similarity measure, facilitated by the 'sentence_transformers' library. The function then returns an array of tuples, each containing a corpus index and a corresponding placeholder score, which is always 0 in this context. The array is sorted by descending similarity, indicating that documents more semantically similar to the query appear first.

                Args:
                    corpus (list of str): The list of document strings to compare against the query.
                    query (str): The query string to compare against the document corpus.

                Returns:
                    list of tuple: A sorted list of tuples where each tuple consists of an index integer and a score float. The index corresponds to the position of the document in the original corpus, and the score is currently a placeholder (0), as sorting is based solely on the negative dot product values.

                Raises:
                    openai.error.OpenAIError: If there is an error while making requests to the OpenAI API.
                    ValueError: If the input types are not as expected or the OpenAI API returns unexpected data.

                Note:
                    This function requires that the 'openai' and 'sentence_transformers' libraries are installed and that an OpenAI API key is configured correctly. The function also alters several global settings of the 'openai' package, which could affect subsequent calls to OpenAI's API in the same session.
            """
            import openai
            from sentence_transformers import util

            openai.api_key = self.open_api_config.get("api_key", "")
            openai.api_base = self.open_api_config.get(
                "api_base", "https://api.openai.com/v1"
            )
            openai.api_type = self.open_api_config.get("api_type", "open_ai")
            openai.api_version = self.open_api_config.get("api_version", "2023-05-15")
            engine = self.open_api_config.get("engine", "text-embedding-ada-002")

            def get_embed(text):
                """

                Generates an embedding for the given text by utilizing OpenAI's Embedding API.

                Args:
                    text (str): The input text for which the embedding is to be generated. Newline characters in the text will be replaced with spaces.

                Returns:
                    List[float]: The embedding vector as a list of floats, retrieved from the OpenAI's Embedding API response for the processed input text.

                Raises:
                    OpenAIError: If the embedding generation fails or API returns an error.
                """
                return openai.Embedding.create(
                    input=[text.replace("\n", " ")], engine=engine
                )["data"][0]["embedding"]

            doc_embeds = [get_embed(i) for i in corpus]
            query = get_embed(query)
            doc_scores = -util.dot_score(doc_embeds, query).cpu().numpy().reshape(-1)
            idx = [(ii, 0) for ii in np.argsort(doc_scores)]
            return idx

        def get_distance_sentbert_bge(corpus, query):
            """


            Calculates the semantic distance between a query and a list of documents using the 'BAAI/bge-large-en-v1.5' Sentence-BERT model.

            This function first checks if there is an already loaded BERT model for retrieval, and if not, loads the 'BAAI/bge-large-en-v1.5' model. It then encodes both the documents and the query into high-dimensional vectors using Sentence-BERT. It computes the dot product between the query and document embeddings to find the semantic similarity. The resulting distances are returned as a list of tuples, where each tuple contains the index of the document and a fixed value of 0 (used for compatibility purposes with other methods), sorted by semantic similarity in ascending order (most relevant first).

            Args:
                corpus (list of str): A list containing the text of the documents to be compared with the query.
                query (str): The query string whose distance is to be computed with each document in the corpus.

            Returns:
                list of tuple: A list of tuples (document_index, fixed_value), where 'document_index' is the index of the document in the 'corpus' list and 'fixed_value' is always 0. The list is sorted by increasing semantic distance, meaning the first element is the closest in meaning to the query.

            Raises:
                ImportError: If the sentence_transformers library is not installed or other required imports are missing.

            """
            from sentence_transformers import SentenceTransformer, util

            if self.retrieval_model is None or self.retrieval_model_name != rank_method:
                self.retrieval_model = SentenceTransformer("BAAI/bge-large-en-v1.5")
                self.retrieval_model_name = rank_method
            doc_embeds = self.retrieval_model.encode(
                [i for i in corpus], normalize_embeddings=True
            )
            query = self.retrieval_model.encode(query, normalize_embeddings=True)
            doc_scores = -util.dot_score(doc_embeds, query).cpu().numpy().reshape(-1)
            idx = [(ii, 0) for ii in np.argsort(doc_scores)]
            return idx

        def get_distance_bge_ranker(corpus, query):
            """

            Computes the ranking of a given corpus of texts against a search query using the BGE reranker model with specified configurations.

                This function takes a list of texts as the corpus and a single text as the query to be matched against the corpus. It initializes and utilizes a pretrained transformer model to rank the corpus in relation to the query. The ranking process involves tokenizing the texts, using the transformer to score them, and sorting based on these scores.

                Args:
                    corpus (List[str]): A list of strings where each string is a document in the corpus to compare against the query.
                    query (str): The query string to be compared against each document in the corpus.

                Returns:
                    List[Tuple[int, int]]: A list of tuples where each tuple contains the index of the document in the corpus and a fixed value of 0, sorted according to their relevance to the query.

                Notes:
                    - The function assumes that the 'BAAI/bge-reranker-large' model from the huggingface model repository is being used.
                    - Prior to calling this function, ensure that PyTorch, the transformers library, and all necessary model dependencies are properly installed and configured.
                    - The function uses the `self.device` attribute to determine the device (CPU/GPU) where the computations will be performed.
                    - The function caches the model and tokenizer within the instance to prevent reloading on subsequent calls if the same ranking method is used.
            """
            from transformers import AutoModelForSequenceClassification, AutoTokenizer

            pairs = [[i, query] for i in corpus]
            if self.retrieval_model is None or self.retrieval_model_name != rank_method:
                tokenizer = AutoTokenizer.from_pretrained("BAAI/bge-reranker-large")
                model = (
                    AutoModelForSequenceClassification.from_pretrained(
                        "BAAI/bge-reranker-large"
                    )
                    .eval()
                    .to(self.device)
                )
                self.retrieval_model = [tokenizer, model]
                self.retrieval_model_name = rank_method
            with torch.no_grad():
                inputs = self.retrieval_model[0](
                    pairs,
                    padding=True,
                    truncation=True,
                    return_tensors="pt",
                    max_length=512,
                ).to(self.device)
                scores = (
                    self.retrieval_model[1](**inputs, return_dict=True)
                    .logits.view(
                        -1,
                    )
                    .float()
                )
            idx = [(ii, 0) for ii in np.argsort(-scores.cpu())]
            return idx

        def get_distance_bge_llmembedder(corpus, query):
            """

            Calculates the similarity scores between a query and a corpus of documents using the embeddings from a language model provided by 'BAAI/llm-embedder' from the Hugging Face library's transformers package. The function is intended for the task of document retrieval by computing the pairwise similarity between the query embedding and each document embedding in the corpus. To ensure performance, the function first checks if the retrieval model is loaded; if not, it loads the tokenizer and model and sets them to evaluation mode. The inputs are tokenized, padded, truncated, and converted into tensors which are then fed to the model to obtain the embeddings. The CLS token embedding is used as the document representation and normalized before similarity is computed using dot product. The similarities are then used to sort the document indices in descending order of relevance to the query.

                Args:
                    corpus (List[str]): A list of documents in the corpus to be compared with the query.
                    query (str): The query string whose relevance is to be evaluated against each document in the corpus.

                Returns:
                    List[Tuple[int, float]]: A sorted list of tuples, where each tuple contains an index of a document in the corpus and its corresponding similarity score to the query. The list is sorted by decreasing similarity score.

                Note:
                    - This function requires a GPU to perform the computation efficiently.
                    - The model and tokenizer must be available through the Huggingface transformers library.
                    - The function assumes that the tokenizer and model are attributes of the class instance (`self`) the function is part of and are accessed accordingly.
                    - The function also assumes the presence of a context manager for torch.no_grad to disable gradient calculations.

            """
            from transformers import AutoModel, AutoTokenizer

            if self.retrieval_model is None or self.retrieval_model_name != rank_method:
                tokenizer = AutoTokenizer.from_pretrained("BAAI/llm-embedder")
                model = (
                    AutoModel.from_pretrained("BAAI/llm-embedder")
                    .eval()
                    .to(self.device)
                )
                self.retrieval_model = [tokenizer, model]
                self.retrieval_model_name = rank_method

            instruction_qa_query = (
                "Represent this query for retrieving relevant documents: "
            )
            instruction_qa_key = "Represent this document for retrieval: "
            queries = [instruction_qa_query + query for _ in corpus]
            keys = [instruction_qa_key + key for key in corpus]
            with torch.no_grad():
                query_inputs = self.retrieval_model[0](
                    queries,
                    padding=True,
                    truncation=True,
                    return_tensors="pt",
                    max_length=512,
                ).to(self.device)
                key_inputs = self.retrieval_model[0](
                    keys,
                    padding=True,
                    truncation=True,
                    return_tensors="pt",
                    max_length=512,
                ).to(self.device)
                query_outputs = self.retrieval_model[1](**query_inputs)
                key_outputs = self.retrieval_model[1](**key_inputs)
                # CLS pooling
                query_embeddings = query_outputs.last_hidden_state[:, 0]
                key_embeddings = key_outputs.last_hidden_state[:, 0]
                # Normalize
                query_embeddings = torch.nn.functional.normalize(
                    query_embeddings, p=2, dim=1
                )
                key_embeddings = torch.nn.functional.normalize(
                    key_embeddings, p=2, dim=1
                )
                similarity = query_embeddings @ key_embeddings.T
            idx = [(ii, 0) for ii in np.argsort(-similarity[0].cpu())]
            return idx

        def get_distance_jinza(corpus, query):
            """


            Calculate the similarity distance between a list of document embeddings and a query embedding using cosine similarity.

            This function first ensures that the appropriate retrieval model is loaded and matches the specified `rank_method`. It encodes the corpus
            and the query text using the `jinaai/jina-embeddings-v2-base-en` embedding model. Then, it calculates the cosine similarity scores
            between the encoded query and each document in the corpus. Finally, it sorts the documents based on their similarity to the query in
            descending order and returns the sorted indices along with a score placeholder (0).

            Args:
                corpus (List[str]): The list of document texts to be compared with the query.
                query (str): The query text that needs to be compared against the documents in the corpus.
                rank_method (str): The embedding model method used for encoding the corpus and query.

            Returns:
                List[Tuple[int, int]]: A list of tuples containing the sorted indices and a score placeholder.
                Each tuple consists of the document index in the original corpus and a placeholder for the similarity score (0).

            Raises:
                ValueError: If the retrieval_model_name does not match the specified rank_method.

            """
            from numpy.linalg import norm

            from transformers import AutoModel

            def cos_sim(a, b):
                """

                Computes the cosine similarity between two vectors.

                    The cosine similarity is calculated as the dot product of the two vectors (a and b)
                    divided by the multiplication of the Euclidean norms (lengths) of the vectors.

                    This function assumes that the input vectors are represented as numpy arrays and
                    that they are already in the appropriate shape for vector multiplication.

                    Args:
                        a (numpy.ndarray): A 1D array representing the first vector.
                        b (numpy.ndarray): A 1D array representing the second vector.

                    Returns:
                        float: The cosine similarity between vector a and vector b, ranging from -1 to 1.

                    Raises:
                        ValueError: If the input arrays are not 1D, or their lengths do not match.
                """
                return (a @ b.T) / (norm(a) * norm(b))

            if self.retrieval_model is None or self.retrieval_model_name != rank_method:
                model = (
                    AutoModel.from_pretrained(
                        "jinaai/jina-embeddings-v2-base-en", trust_remote_code=True
                    )
                    .eval()
                    .to(self.device)
                )
                self.retrieval_model = model
                self.retrieval_model_name = rank_method

            doc_embeds = self.retrieval_model.encode(corpus)
            query = self.retrieval_model.encode(query)
            doc_scores = cos_sim(doc_embeds, query)
            idx = [(ii, 0) for ii in np.argsort(-doc_scores)]
            return idx

        def get_distance_voyageai(corpus, query):
            """

            Computes the semantic similarity between a query and a corpus of documents using Voyage.ai's embedding model and the dot product similarity measure from the Sentence Transformers library. This function generates embeddings for each document in the corpus and the query, then calculates the similarity score for each document with respect to the query. The scores are used to sort the documents in descending order of relevance to the query, i.e., the higher the score, the more relevant the document is considered to be. The sorted indices of the documents along with a dummy score (always 0 in this implementation) are returned as a list of tuples.

            Args:
                corpus (List[str]): A list of strings representing the documents.
                query (str): The query string against which document similarity is to be computed.

            Returns:
                List[Tuple[int, int]]: A sorted list of tuples containing the indices and dummy scores of the documents in the corpus, arranged in order of their relevance to the query based on the similarity scores.

            Remarks:
                This function requires `voyageai` and `sentence_transformers` libraries.
                The `voyageai.api_key` must be set before calling this function, which authenticates the user against Voyage.ai API.
                The `cpu()` call on `dot_score` suggests that this function is compatible with PyTorch tensors; therefore, the embeddings are expected to be PyTorch tensors.
            """
            import voyageai
            from sentence_transformers import util

            voyageai.api_key = self.open_api_config.get("voyageai_api_key", "")

            def get_embed(text):
                """


                Computes the embedding for a given piece of text using the Voyage AI model.

                This function takes in a string of text and returns the computed embedding from the specified Voyage AI model.

                Args:
                    text (str): The text for which the embedding should be computed.

                Returns:
                    np.ndarray: An array representing the embedding of the input text.

                Raises:
                    Exception: If the embedding model fails to compute the embedding or if the `voyageai` module encounters an issue.
                """
                return voyageai.get_embedding(text, model="voyage-01")

            doc_embeds = [get_embed(i) for i in corpus]
            query = get_embed(query)
            doc_scores = -util.dot_score(doc_embeds, query).cpu().numpy().reshape(-1)
            idx = [(ii, 0) for ii in np.argsort(doc_scores)]
            return idx

        def get_distance_cohere(corpus, query):
            """

            Uses the Cohere API to rerank a given corpus of documents based on their semantic relevance to a query string.

                This function interacts with the Cohere API, requiring a valid API key to be provided. It reranks the documents in the given corpus
                based on how semantically similar they are to the input query. The corpus is a list of strings, each representing a document. The query
                is a string that represents the search term. The function then maps the reranked results to their original indices in the corpus
                list and returns a list of tuples, with each tuple containing an index (representing the original position of the document in the
                corpus) and a placeholder value '0' (which currently does not represent any score or rank metric).

                Arguments:
                    corpus (list of str): A list of documents (strings) to be reranked.
                    query (str): The query string used for reranking the corpus.

                Returns:
                    list of tuples: Each tuple contains an index (int) of the document in the original corpus and a placeholder value '0'.

            """
            import cohere

            api_key = self.open_api_config.get("cohere_api_key", "")
            co = cohere.Client(api_key)
            results = co.rerank(
                model="rerank-english-v2.0", query=query, documents=corpus, top_n=20
            )
            c_map = {jj: ii for ii, jj in enumerate(corpus)}
            doc_rank = [c_map[ii.document["text"]] for ii in results]
            idx = [(ii, 0) for ii in doc_rank]
            return idx

        def get_distance_longllmlingua(corpus, query):
            """

            Calculate a contextually modified distance measure between the query and each document in the corpus.

                This function computes a personalized log-likelihood metric for each document
                in the corpus in relation to a given query. It uses the perplexity measure returned
                by `get_condition_ppl` and modifies it based on the document's length and
                a condition. It finally sorts the documents based on the condition.

                Args:
                    corpus (list of str): A list of documents to be evaluated.
                    query (str): The query string used for comparison with the documents.

                Returns:
                    list of tuple: A sorted list of tuples where each tuple contains (index, modified_perplexity_score)
                    for the documents. The index corresponds to the position of the document in the original corpus.

                Raises:
                    NameError: If `condition_in_question` is undefined.
                    AttributeError: If `self.get_condition_ppl` method is not defined in the current context.

                Note:
                    The sorting direction is determined by `condition_in_question`. If this variable is equal to 'none',
                    the score is multiplied by -1, otherwise by 1.

                    The variable `context_tokens_length` must be a list of lengths corresponding to each document in `corpus`.
                    The actual implementation of `get_condition_ppl` is not provided and should be
                    implemented separately. It is expected to return the perplexity of the document given a contextuous addition.


            """
            context_ppl = [
                self.get_condition_ppl(
                    d,
                    query
                    + " We can get the answer to this question in the given documents.",
                    condition_in_question,
                )
                - dl * 2 / 250 * 0
                for d, dl in zip(corpus, context_tokens_length)
            ]
            sort_direct = -1 if condition_in_question == "none" else 1
            ys = sorted(enumerate(context_ppl), key=lambda x: sort_direct * x[1])
            return ys

        method = None
        if rank_method == "bm25":
            method = get_distance_bm25
        elif rank_method == "gzip":
            method = get_distance_gzip
        elif rank_method == "sentbert":
            method = get_distance_sentbert
        elif rank_method == "openai":
            method = get_distance_openai
        elif rank_method in ["longllmlingua", "llmlingua"]:
            method = get_distance_longllmlingua
        elif rank_method == "bge":
            method = get_distance_sentbert_bge
        elif rank_method == "bge_reranker":
            method = get_distance_bge_ranker
        elif rank_method == "bge_llmembedder":
            method = get_distance_bge_llmembedder
        elif rank_method == "jinza":
            method = get_distance_jinza
        elif rank_method == "voyageai":
            method = get_distance_voyageai
        elif rank_method == "cohere":
            method = get_distance_cohere
        return method(context, question)
