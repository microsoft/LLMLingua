"""

Module: prompt_compressor

This module contains the `PromptCompressor` class designed for compressing the context of prompts
to effectively reduce the token count for large language models (LLMs) while preserving the critical information needed for the model to generate high-quality responses.
The `PromptCompressor` class loads LLMs and provides several methods to handle different aspects of prompt compression.

Class `PromptCompressor`:
 - __init__(self, model_name: str='NousResearch/Llama-2-7b-hf', device_map: str='cuda', model_config: dict={}, open_api_config: dict={}):
    Initializes the `PromptCompressor` instance by loading the model as specified by `model_name`, assigns a device map,
    sets model configurations, and handles open API configurations if available.

 - load_model(self, model_name: str, device_map: str='cuda', model_config: dict={}):
    Loads the language model based on the given `model_name`, `device_map`, and `model_config`. It also configures the tokenizer
    settings such as padding side and pad token ID.

 - get_ppl(self, text: str, granularity: str='sentence', input_ids=None, attention_mask=None, past_key_values=None, return_kv=False, end=None, condition_mode: str='none', condition_pos_id: int=0):
    Calculates the perplexity of a given text with an optional specified granularity and tokenizing options.
    It handles past key values for more efficient calculations and allows specifying a condition mode for the calculation.

 - compress_prompt(self, context: List[str], instruction: str='', question: str='', ratio: float=0.5, target_token: float=-1, iterative_size: int=200, force_context_ids: List[int]=None, force_context_number: int=None, use_sentence_level_filter: bool=False, use_context_level_filter: bool=True, use_token_level_filter: bool=True, keep_split: bool=False, keep_first_sentence: int=0, keep_last_sentence: int=0, keep_sentence_number: int=0, high_priority_bonus: int=100, context_budget: str='+100', token_budget_ratio: float=1.4, condition_in_question: str='none', reorder_context: str='original', dynamic_context_compression_ratio: float=0.0, condition_compare: bool=False, add_instruction: bool=False, rank_method: str='llmlingua', concate_question: bool=True):
    Main method that compresses the input context based on various parameters such as token count targets,
    compression ratio settings, filtering preferences at different levels (sentence or context), and ranking methods for context importance.

 - get_token_length(self, text: str, add_special_tokens: bool=True):
    Returns the token length of the given text, taking into account whether special tokens should be added or not.

 - get_condition_ppl(self, text: str, question: str, condition_in_question: str='none', granularity: str='sentence'):
    Computes perplexity for text with the condition in the question taken into consideration.

 - get_rank_results(self, context: list, question: str, rank_method: str, condition_in_question: str, context_tokens_length: list):
    Obtains ranking results for the given context based on the specified ranking method and condition in the question.
    This function uses different retrieval techniques based on the provided `rank_method` parameter.


Note: Alongside the methods mentioned above, several internal utility methods are used within the `PromptCompressor`.
The class also includes functionality to recover the original response from compressed prompts using the `recover` method.

Note: Documentation automatically generated by https://undoc.ai
"""
import bisect
from collections import defaultdict
from typing import List

import numpy as np
import torch

import nltk
import tiktoken
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer

encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")


class PromptCompressor:
    """

    Class to compress prompts before feeding them into a language model to manage token limitations. It includes various methods to control compression levels and preserve the relevance of the content. The class supports different ranking methods and dynamic compression strategies to effectively summarize contexts and questions for input to large language models.

    Attributes:
        retrieval_model (Optional): An optional model used for ranking documents in context retrieval.
        retrieval_model_name (Optional[str]): The name of the model used for retrieval, if applicable.
        open_api_config (dict): Configuration dictionary for OpenAI-related API interactions.
        cache_bos_num (int): Cache the number of beginning-of-sentence tokens used during prediction.
        prefix_bos_num (int): Prefix BOS token number used during prediction.
        tokenizer (AutoTokenizer): Tokenizer from the Transformers library for processing input text.
        model (AutoModelForCausalLM): The language generation model.
        context_idxs (list): Indexes of contexts used for dynamic prompt compression.
        max_position_embeddings (int): Maximum sequence length that the model accepts.

    Methods:
        __init__(): Initializes an instance of PromptCompressor with the given model, device map, and configuration options.
        load_model(): Loads the specified language model and tokenizer with the provided configuration.
        get_ppl(): Computes the perplexity of a given text segment based on the provided model and tokenizer.
        __call__(): Compresses the prompt by delegating to compress_prompt() when the instance is called like a function.
        compress_prompt(): Compresses the input context and question based on the specified parameters, using different levels of filtering and dynamic compression settings.
        get_token_length(): Determines the number of tokens that a given text would occupy after tokenization.
        get_condition_ppl(): Calculates the perplexity of a given text, with the condition applied before or after the text segment.
        get_dynamic_compression_ratio(): Calculates dynamic compression ratios for iterative compression of prompts.
        control_context_budget(): Controls the token budget for various contexts, selecting a subset based on target token limits and ranking methods.
        control_sentence_budget(): Controls the sentence-level budget for token usage, deciding which sentences to retain.
        get_compressed_input(): Helps in the iterative compression process by selecting tokens to keep based on loss thresholds.
        get_estimate_threshold_base_distribution(): Estimates the threshold for selecting tokens to keep based on the desired compression ratio and model's predictions.
        iterative_compress_prompt(): Progressively compresses the prompt through an iterative process with specified parameters, adjusting for keep flags and dynamic ratios.
        recover(): Reconstructs the response from the compressed prompt to the original prompt's context, aligning the tokens for the user's reference.
        get_rank_results(): Retrieves ranking results for context sentences or documents using various ranking methods to prioritize useful information.
    """

    def __init__(
            self,
            model_name: str = "NousResearch/Llama-2-7b-hf",
            device_map: str = "cuda",
            model_config: dict = {},
            open_api_config: dict = {},
    ):
        """


            Initializes an instance of the object with the given parameters.

            This method is responsible for setting up the model based on the provided model name and device map, as well as initializing various configuration parameters.

            Parameters:
            -----------
            model_name : str, optional
                The name of the model to be loaded. Defaults to 'NousResearch/Llama-2-7b-hf'.
            device_map : str, optional
                The device on which the model is to be deployed. Defaults to 'cuda' for GPU support.
            model_config : dict, optional
                A dictionary containing the configuration settings for the model.
            open_api_config : dict, optional
                A dictionary containing configuration for open API.

            Attributes:
            -----------
            retrieval_model : NoneType
                Initially set to None. To be defined for retrieving documents or information.
            retrieval_model_name : NoneType
                Initially set to None. To store the name of the retrieval model if one is used.
            open_api_config : dict
                Stores the open API configuration passed as a parameter.
            cache_bos_num : int
                The number of beginning-of-sentence tokens to cache, pre-set to 10.
            prefix_bos_num : int
                The number of beginning-of-sentence tokens that are prefixed to the input, pre-set to 100.

            The `__init__` method is typically called when an instance of the class is created and is not meant to be invoked directly by the user.

        """
        self.load_model(model_name, device_map, model_config)
        self.retrieval_model = None
        self.retrieval_model_name = None
        self.open_api_config = open_api_config
        self.cache_bos_num = 10
        self.prefix_bos_num = 100

    def load_model(
            self, model_name: str, device_map: str = "cuda", model_config: dict = {}
    ):
        """


            Loads a language model and its tokenizer into the class instance based on specified configuration.

            The function is designed to set up a model for causal language modeling by acquiring pre-trained settings,
            a tokenizer, and model weights which can be fine-tuned, using the model name provided. It also manages
            device placement for the model and adapts tokenizer's padding direction based on configurations.

            This loading process ensures that the model is compatible with the expected hardware (like CPU or CUDA)
            and loads it accordingly. It accommodates specific configurations such as ignoring mismatched sizes,
            and offloading capabilities to optimize memory usage on the specified device.

            Args:
                model_name (str): The name or path of the pre-trained model to be loaded.
                device_map (str): The device identifier where the model will be loaded. Defaults to 'cuda'.
                model_config (dict): A dictionary of configurations to apply to the model.
                    Recognized keys include 'trust_remote_code', 'pad_to_left', and others pertaining to the
                    model's specific implementation and requirements. A particularly important key is
                    'trust_remote_code', which defaults to True if not provided, and is essential for the
                    AutoConfig to load a remote configuration securely.

            Modifies:
                - Assigns the loaded model and tokenizer to the instance variables 'model' and 'tokenizer'.
                - Sets 'device' as an instance variable indicating the device on which the model is loaded.
                - 'context_idxs' is modified to an empty list as preparation for any further processes.
                - Sets the 'max_position_embeddings' from the model configuration to an instance variable.

            Raises:
                This function does not explicitly raise exceptions but may do so as a part of
                the underlying library calls (e.g., HuggingFace Transformers) when loading models
                or tokenizers, particularly if model_name is invalid or device_map is misconfigured.


        """
        trust_remote_code = model_config.get("trust_remote_code", True)
        if "trust_remote_code" not in model_config:
            model_config["trust_remote_code"] = trust_remote_code
        config = AutoConfig.from_pretrained(model_name, trust_remote_code=trust_remote_code)
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=trust_remote_code)
        if model_config.get("pad_to_left", True):
            tokenizer.padding_side = "left"
            tokenizer.pad_token_id = (
                config.pad_token_id if config.pad_token_id else tokenizer.eos_token_id
            )
        self.device = (
            device_map if any(key in device_map for key in ["cuda", "cpu"]) else "cuda"
        )
        if "cuda" in device_map or "cpu" in device_map:
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype="auto" if device_map == "cuda" else torch.float32,
                device_map=device_map,
                config=config,
                ignore_mismatched_sizes=True,
                **model_config
            )
        else:
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                device_map=device_map,
                torch_dtype="auto",
                pad_token_id=tokenizer.pad_token_id,
                offload_folder="/tmp/offload",
                offload_state_dict=True,
                cache_dir="/tmp/cache",
                **model_config
            )
        self.tokenizer = tokenizer
        self.model = model
        self.context_idxs = []
        self.max_position_embeddings = config.max_position_embeddings

    def get_ppl(
            self,
            text: str,
            granularity: str = "sentence",
            input_ids=None,
            attention_mask=None,
            past_key_values=None,
            return_kv=False,
            end=None,
            condition_mode: str = "none",
            condition_pos_id: int = 0,
    ):
        """

        def get_ppl(self, text: str, granularity: str='sentence', input_ids=None, attention_mask=None, past_key_values=None, return_kv=False, end=None, condition_mode: str='none', condition_pos_id: int=0):

                Calculates the perplexity of a given text using the model associated with the self instance.

                Perplexity is a measurement of how well a probability distribution or probability model predicts a sample. It may be used to compare probability models. A low perplexity indicates the probability distribution is good at predicting the sample.

                Args:
                    text (str): The input text for which the perplexity is to be calculated.
                    granularity (str, optional): The granularity at which the perplexity should be calculated ('sentence' for the perplexity of the entire text or 'token' for per-token perplexity). Defaults to 'sentence'.
                    input_ids (optional): Precomputed input ids for the text. If not provided, will be computed from the raw text. Defaults to None.
                    attention_mask (optional): Precomputed attention mask for the text. If not provided, will be computed from the raw text. Defaults to None.
                    past_key_values (optional): Tuple of key/value pairs that represent the past state of the model. Defaults to None.
                    return_kv (bool, optional): If set to True, returns updated key/value pairs representing the new state after processing the input text. Defaults to False.
                    end (optional): Index after which sequence will be truncated. If None, sequences are not truncated. Defaults to None.
                    condition_mode (str, optional): Specifies where to apply a conditional operation in the sequence ('before', 'after', or 'none'). Defaults to 'none'.
                    condition_pos_id (int, optional): The token index used as a cutoff point for the conditional operation specified by condition_mode. Defaults to 0.

                Returns:
                    If return_kv is False, returns the average loss calculated as the perplexity of the input `text`. The return type is a Tensor if granularity is 'token' or a float if granularity is 'sentence'.
                    If return_kv is True, returns a tuple containing the calculated loss and the updated past_key_values.

                Note:
                    The function relies on an instance of a model that should be previously loaded and associated with the 'self'. The model should be of a class capable of processing the text input and returning logits.
                    The actual calculation of perplexity assumes the use of a model that captures the statistical properties of the language such as a transformer-based language model.
        """
        if input_ids is None:
            tokenized_text = self.tokenizer(text, return_tensors="pt")
            input_ids = tokenized_text["input_ids"].to(self.device)
            attention_mask = tokenized_text["attention_mask"].to(self.device)
        if past_key_values is not None:
            past_length = past_key_values[0][0].shape[2]
        else:
            past_length = 0
        if end is None:
            end = input_ids.shape[1]
        end = min(end, past_length + self.max_position_embeddings)
        with torch.no_grad():
            response = self.model(
                input_ids[:, past_length:end],
                attention_mask=attention_mask[:, :end],
                past_key_values=past_key_values,
                use_cache=True,
            )
            past_key_values = response.past_key_values

        loss_fct = torch.nn.CrossEntropyLoss(reduction="none")
        shift_logits = response.logits[..., :-1, :].contiguous()
        shift_labels = input_ids[..., past_length + 1: end].contiguous()
        # Flatten the tokens
        active = (attention_mask[:, past_length:end] == 1)[..., :-1].view(-1)
        active_logits = shift_logits.view(-1, shift_logits.size(-1))[active]
        active_labels = shift_labels.view(-1)[active]
        loss_fct = torch.nn.CrossEntropyLoss(reduction="none")
        loss = loss_fct(active_logits, active_labels)
        if condition_mode == "before":
            loss = loss[:condition_pos_id]
        elif condition_mode == "after":
            loss = loss[condition_pos_id:]
        res = loss.mean() if granularity == "sentence" else loss
        return (res, past_key_values) if return_kv else res

    def __call__(self, *args, **kwargs):
        """

        ```
            def __call__(self, *args, **kwargs):

                Calls the instance's `compress_prompt` method.

                When the instance of the class is called like a function, the Python interpreter automatically invokes the `__call__` method. This method in turn forwards all received arguments to the `compress_prompt` method of the instance, which is expected to be defined elsewhere within the class.

                Parameters:
                ----------
                *args : tuple
                    Variable length argument list to be passed to the `compress_prompt` method.

                **kwargs : dict
                    Arbitrary keyword arguments to be passed to the `compress_prompt` method.

                Returns:
                -------
                The return value from the `compress_prompt` method.

                Note:
                -----
                The actual behavior and return type completely depend on the implementation of the `compress_prompt` method, and thus cannot be inferred from this documentation alone.
        ```
        """
        return self.compress_prompt(*args, **kwargs)

    def compress_prompt(
            self,
            context: List[str],
            instruction: str = "",
            question: str = "",
            ratio: float = 0.5,
            target_token: float = -1,
            iterative_size: int = 200,
            force_context_ids: List[int] = None,
            force_context_number: int = None,
            use_sentence_level_filter: bool = False,
            use_context_level_filter: bool = True,
            use_token_level_filter: bool = True,
            keep_split: bool = False,
            keep_first_sentence: int = 0,
            keep_last_sentence: int = 0,
            keep_sentence_number: int = 0,
            high_priority_bonus: int = 100,
            context_budget: str = "+100",
            token_budget_ratio: float = 1.4,
            condition_in_question: str = "none",
            reorder_context: str = "original",
            dynamic_context_compression_ratio: float = 0.0,
            condition_compare: bool = False,
            add_instruction: bool = False,
            rank_method: str = "llmlingua",
            concate_question: bool = True,
    ):
        """

        def compress_prompt(self, context: List[str], instruction: str='', question: str='', ratio: float=0.5, target_token: float=-1, iterative_size: int=200, force_context_ids: List[int]=None, force_context_number: int=None, use_sentence_level_filter: bool=False, use_context_level_filter: bool=True, use_token_level_filter: bool=True, keep_split: bool=False, keep_first_sentence: int=0, keep_last_sentence: int=0, keep_sentence_number: int=0, high_priority_bonus: int=100, context_budget: str='+100', token_budget_ratio: float=1.4, condition_in_question: str='none', reorder_context: str='original', dynamic_context_compression_ratio: float=0.0, condition_compare: bool=False, add_instruction: bool=False, rank_method: str='llmlingua', concate_question: bool=True):

        """
        if not context:
            context = [" "]
        if isinstance(context, str):
            context = [context]
        assert not (
                rank_method == "longllmlingua" and not question
        ), "In the LongLLMLingua, it is necessary to set a question."
        if condition_compare and "_condition" not in condition_in_question:
            condition_in_question += "_condition"
        if rank_method == "longllmlingua":
            if condition_in_question == "none":
                condition_in_question = "after"
        elif rank_method == "llmlingua":
            condition_in_question = (
                "none"
                if "_condition" not in condition_in_question
                else "none_condition"
            )
        origin_tokens = len(
            encoding.encode("\n\n".join([instruction] + context + [question]).strip())
        )
        context_tokens_length = [self.get_token_length(c) for c in context]
        instruction_tokens_length, question_tokens_length = self.get_token_length(
            instruction
        ), self.get_token_length(question)
        if target_token == -1:
            target_token = (
                    (
                            instruction_tokens_length
                            + question_tokens_length
                            + sum(context_tokens_length)
                    )
                    * (1 - ratio)
                    - instruction_tokens_length
                    - (question_tokens_length if concate_question else 0)
            )
        condition_flag = "_condition" in condition_in_question
        condition_in_question = condition_in_question.replace("_condition", "")

        if len(context) > 1 and use_context_level_filter:
            context, dynamic_ratio = self.control_context_budget(
                context,
                context_tokens_length,
                target_token,
                force_context_ids,
                force_context_number,
                question,
                condition_in_question,
                reorder_context=reorder_context,
                dynamic_context_compression_ratio=dynamic_context_compression_ratio,
                rank_method=rank_method,
                context_budget=context_budget,
            )
        else:
            dynamic_ratio = [0.0] * len(context)

        if use_sentence_level_filter:
            context = self.control_sentence_budget(
                context,
                target_token,
                keep_first_sentence=keep_first_sentence,
                keep_last_sentence=keep_last_sentence,
                keep_sentence_number=keep_sentence_number,
                high_priority_bonus=high_priority_bonus,
                token_budget_ratio=token_budget_ratio,
                question=question,
                condition_in_question=condition_in_question,
                rank_method=rank_method,
            )

        if condition_flag:
            prefix = question + "\n\n" + instruction if add_instruction else question
            if (
                    self.get_token_length(prefix) + 2 + iterative_size * 2
                    > self.max_position_embeddings
            ):
                tokens = self.tokenizer(prefix, add_special_tokens=False).input_ids
                prefix = self.tokenizer.decode(
                    tokens[: self.prefix_bos_num]
                    + tokens[
                      len(tokens)
                      - self.max_position_embeddings
                      + 2
                      + self.prefix_bos_num
                      + 2 * iterative_size:
                      ]
                )
            start = self.get_token_length(prefix) + 2
            context = [prefix] + context
        else:
            start = 0

        if use_token_level_filter:
            context = self.iterative_compress_prompt(
                context,
                target_token,
                iterative_size=iterative_size,
                keep_split=keep_split,
                start=start,
                dynamic_ratio=dynamic_ratio,
                condition_compare=condition_compare,
            )
            compressed_prompt = (
                self.tokenizer.batch_decode(context[0])[0]
                .replace("<s> ", "")
                .replace("<s>", "")
            )
        else:
            compressed_prompt = "\n\n".join(context)

        res = []
        if instruction:
            res.append(instruction)
        if compressed_prompt.strip():
            res.append(compressed_prompt)
        if question and concate_question:
            res.append(question)

        compressed_prompt = "\n\n".join(res)

        compressed_tokens = len(encoding.encode(compressed_prompt))
        saving = (origin_tokens - compressed_tokens) * 0.06 / 1000
        return {
            "compressed_prompt": compressed_prompt,
            "origin_tokens": origin_tokens,
            "compressed_tokens": compressed_tokens,
            "ratio": f"{origin_tokens / compressed_tokens:.1f}x",
            "saving": f", Saving ${saving:.1f} in GPT-4.",
        }

    def get_token_length(self, text: str, add_special_tokens: bool = True):
        """

        def get_token_length(self, text: str, add_special_tokens: bool=True):

            Returns the length of tokens for the given text when processed by the tokenizer.

            This function calculates the number of tokens generated by the tokenizer after processing the input text. It accounts for special tokens that may be added to the sequence, like start or end tokens, based on the tokenizer's configuration and the `add_special_tokens` parameter.

            Parameters:
                text (str): The text to be tokenized and measured.
                add_special_tokens (bool, optional): Flag indicating whether to include special tokens
                    (like start or end tokens) in the tokenization process. Defaults to True.

            Returns:
                int: The number of tokens in the tokenized representation of the input text, including
                    any special tokens if `add_special_tokens` is set to True.


        """
        return len(
            self.tokenizer(text, add_special_tokens=add_special_tokens).input_ids
        )

    def get_condition_ppl(
            self,
            text: str,
            question: str,
            condition_in_question: str = "none",
            granularity: str = "sentence",
    ):
        """

        Calculates the perplexity of a given text in relation to a question, under specific conditions.

            This method computes the perplexity (ppl) value which is a measure of how well a probabilistic
            model predicts a sample. It is used to compare the text's relevance or fluency concerning a
            condition specified by a question. The method also takes into account the condition's position
            relative to the text (either before or after the text).

            Parameters:
                text (str): The text for which perplexity is to be calculated.
                question (str): The question used to condition the perplexity computation.
                condition_in_question (str, optional): A string that specifies the position of the condition
                    in relation to the text. It can be 'none', 'before', or 'after'. By default, it is 'none',
                    which means the perplexity is calculated without considering the question as a condition.
                granularity (str, optional): A string that specifies the granularity at which perplexity is
                    calculated. It could be 'sentence' or other levels of granularity. The default is 'sentence'.

            Returns:
                float: The calculated perplexity value of the text under the given condition.

            Raises:
                ValueError: If the condition_in_question parameter is not one of 'none', 'before', or 'after'.
        """
        if condition_in_question == "none":
            return self.get_ppl(text, granularity=granularity)
        elif condition_in_question == "before":
            return self.get_ppl(
                question + text,
                granularity=granularity,
                condition_mode="after",
                condition_pos_id=self.get_token_length(question) - 1,
            )
        elif condition_in_question == "after":
            return self.get_ppl(
                text + question,
                granularity=granularity,
                condition_mode="after",
                condition_pos_id=self.get_token_length(text) - 1,
            )

    def get_dynamic_compression_ratio(
            self,
            context: list,
            target_token: float,
            iterative_size: int,
            dynamic_ratio: list,
            start: int,
    ):
        """

        def get_dynamic_compression_ratio(self, context: list, target_token: float, iterative_size: int, dynamic_ratio: list, start: int):
        """

        def get_ratio(base: float, delta: float):
            """

            def get_ratio(base: float, delta: float) -> float:
            """
            return max(min(1, base + delta), 0)

        context_length = [self.get_token_length(ii, False) + 2 for ii in context]
        if start:
            context_length = context_length[1:]
        tau = target_token / (sum(context_length) + 1)
        res, idx, last, last_target = [], 0, 1, []
        while idx < len(context_length):
            if last + context_length[idx] >= iterative_size:
                last_target.append(
                    (iterative_size - last, get_ratio(tau, dynamic_ratio[idx]))
                )
                res.append(last_target)
                last = last + context_length[idx] - iterative_size
                if last > iterative_size:
                    k = last // iterative_size
                    res.extend(
                        [[(iterative_size, get_ratio(tau, dynamic_ratio[idx]))]] * k
                    )
                    last -= k * iterative_size

                last_target = (
                    [(last, get_ratio(tau, dynamic_ratio[idx]))] if last else []
                )
            else:
                last += context_length[idx]
                last_target.append(
                    (context_length[idx], get_ratio(tau, dynamic_ratio[idx]))
                )
            idx += 1
        if last_target:
            res.append(last_target)
        return res

    def control_context_budget(
            self,
            context: List[str],
            context_tokens_length: List[int],
            target_token: float,
            force_context_ids: List[int] = None,
            force_context_number: int = None,
            question: str = "",
            condition_in_question: str = "none",
            reorder_context: str = "original",
            dynamic_context_compression_ratio: float = 0.0,
            rank_method: str = "longllmlingua",
            context_budget: str = "+100",
    ):
        """

        def control_context_budget(self, context: List[str], context_tokens_length: List[int], target_token: float, force_context_ids: List[int]=None, force_context_number: int=None, question: str='', condition_in_question: str='none', reorder_context: str='original', dynamic_context_compression_ratio: float=0.0, rank_method: str='longllmlingua', context_budget: str='+100'):
        """
        if force_context_ids is not None:
            return [context[ii] for ii in force_context_ids]
        demostrations_sort = self.get_rank_results(
            context,
            question,
            rank_method,
            condition_in_question,
            context_tokens_length,
        )

        if target_token < 0:
            target_token = 100
        target_token = eval("target_token" + context_budget)
        res = []
        used = force_context_ids if force_context_ids is not None else []

        self.context_idxs.append([x for idx, (x, _) in enumerate(demostrations_sort)])
        for idx, _ in demostrations_sort:
            if idx >= len(context_tokens_length):
                continue
            target_token -= context_tokens_length[idx]
            if idx not in used:
                used.append(idx)
            if target_token < 0 or (
                    force_context_number is not None and len(res) >= force_context_number
            ):
                break
        original_used = used
        if reorder_context == "original":
            used = sorted(used)
        elif reorder_context == "two_stage":
            l, r = [_ for idx, _ in enumerate(used) if idx % 2 == 0], [
                _ for idx, _ in enumerate(used) if idx % 2 == 1
            ]
            used = l + r[::-1]

        if dynamic_context_compression_ratio > 0:
            N = len(used)
            if condition_in_question:
                rank = [
                    i
                    for i, _ in self.get_rank_results(
                        context,
                        question,
                        "longllmlingua",
                        "after",
                        context_tokens_length,
                    )
                ]
                used = sorted(used, key=lambda x: rank.index(x))
            dynamic_ratio = [
                                i * (abs(dynamic_context_compression_ratio) / (N - 1)) if N > 1 else 0
                                for i in range(-(N - 1), N, 2)
                            ][::-1]
            dynamic_ratio_map = {i: j for i, j in zip(original_used, dynamic_ratio)}
            dynamic_ratio = [dynamic_ratio_map[i] for i in used]
        else:
            dynamic_ratio = [0.0] * len(used)

        res = [context[idx] for idx in used if idx < len(context)]
        return res, dynamic_ratio

    def control_sentence_budget(
            self,
            context: List[str],
            target_token: float,
            keep_first_sentence: int = 0,
            keep_last_sentence: int = 0,
            keep_sentence_number: int = 0,
            high_priority_bonus: int = 100,
            token_budget_ratio: float = 1.4,
            question: str = "",
            condition_in_question: str = "none",
            rank_method: str = "longllmlingua",
    ):
        """

        def control_sentence_budget(self, context, target_token, keep_first_sentence=0, keep_last_sentence=0, keep_sentence_number=0, high_priority_bonus=100, token_budget_ratio=1.4, question='', condition_in_question='none', rank_method='longllmlingua'):


            Reduces the given context to fit a specified token budget by selectively keeping sentences according to the specified ranking method.

            The method processes a list of string representing the context, applies a natural language understanding model to rank each sentence's relevance to a given query, and keeps the most relevant sentences within the token budget constraint.

            Parameters:
                context (List[str]): List of context strings, where each string is a set of sentences from a document.
                target_token (float): Initial target token budget which will be adjusted by the token_budget_ratio.
                keep_first_sentence (int, optional): Number of initial sentences in each context string to prioritize. Defaults to 0.
                keep_last_sentence (int, optional): Number of final sentences in each context string to prioritize. Defaults to 0.
                keep_sentence_number (int, optional): Number of sentences to keep from each document irrespective of their relevance.
                high_priority_bonus (int, optional): The priority score to add for sentences that are kept based on their position or explicit selection.
                token_budget_ratio (float, optional): Ratio to scale the target_token budget for retaining sentences. Defaults to 1.4.
                question (str, optional): The query string that sentences are ranked against for relevance. Defaults to an empty string.
                condition_in_question (str, optional): Specific condition in question to consider in the ranking. Defaults to 'none'.
                rank_method (str, optional): The ranking method name to be used to score and sort sentences. Defaults to 'longllmlingua'.

            Returns:
                List[str]: A list of context strings, where less relevant sentences have been removed to meet the token_budget.

            Raises:
                ValueError: If the rank_method provided is not recognized or is unsupported.


        """

        def keep_sentence(dem_idx: int, sent_keep: int):
            """

            Calculates the perplexity of sentences within a document and applies a priority bonus to the sentences with the lowest perplexity scores, effectively 'keeping' them.

            This function sorts the perplexity scores of sentences within a document identified by 'dem_idx'. It then increments the perplexity score of each sentence by a 'high_priority_bonus', but only for the number of sentences specified by 'sent_keep'. This can be used to prioritize certain sentences in subsequent processing, such as summarization or information retrieval tasks.

            Args:
                dem_idx (int): An index that identifies the document from which sentences should be evaluated.
                sent_keep (int): The number of sentences to apply the 'high_priority_bonus' to. These are selected based on having the lowest perplexity scores.

            Note:
                - This function assumes that 'dem_g' is a global variable accessible within the function scope, which contains the perplexity scores for all sentences across all documents.
                - 'sentence_ppl' is also assumed to be a global variable holding individual sentence perplexity scores.
                - 'high_priority_bonus' is a predefined value that is added to the perplexity scores of the sentences being prioritized ('kept').

            Returns:
                None: This function modifies the perplexity scores in place and does not return any value.

            Raises:
                IndexError: If 'dem_idx' is out of bounds for the 'dem_g' list, an IndexError exception will be raised.
                TypeError: If 'dem_idx' or 'sent_keep' is not passed as an integer, a TypeError will be raised.
            """
            idxs = sorted(dem_g[dem_idx], key=lambda x: sentence_ppl[x])[:sent_keep]
            for idx in idxs:
                sentence_ppl[idx] += high_priority_bonus

        sentences = [nltk.sent_tokenize(c) for c in context]
        dem_g, s2de, idx = defaultdict(set), defaultdict(int), 0
        for idx_d, s in enumerate(sentences):
            for _ in s:
                dem_g[idx_d].add(idx)
                s2de[idx] = idx_d
                idx += 1

        context_sentences = [s for ii in sentences for s in ii]
        sentence_tokens_length = [
            self.get_token_length(sentence) for sentence in context_sentences
        ]
        N = len(context_sentences)
        flags = list(range(len(context_sentences)))
        if len(sentence_tokens_length) == 1:
            return context
        if rank_method == "longllmlingua":
            sentence_ppl = [
                self.get_condition_ppl(sentence, question, condition_in_question)
                .cpu()
                .numpy()
                .item()
                for sentence in context_sentences
            ]
            if keep_first_sentence:
                sentence_ppl[:keep_first_sentence] = [
                    ii + high_priority_bonus
                    for ii in sentence_ppl[:keep_first_sentence]
                ]
            if keep_last_sentence:
                sentence_ppl[-keep_last_sentence:] = [
                    ii + high_priority_bonus
                    for ii in sentence_ppl[-keep_last_sentence:]
                ]
            if keep_sentence_number:
                for dem_idx in range(len(sentences)):
                    keep_sentence(dem_idx, keep_sentence_number)
            sort_direct = -1 if condition_in_question == "none" else 1
            sent_sort = sorted(
                enumerate(sentence_ppl), key=lambda x: sort_direct * x[1]
            )
        else:
            sent_sort = self.get_rank_results(
                context_sentences,
                question,
                rank_method,
                condition_in_question,
                [0] * len(context_sentences),
            )

        sentence_flags = [False] * N
        if target_token < 0:
            target_token = 100
        target_token *= token_budget_ratio
        res = []
        for idx, _ in sent_sort:
            idx = flags[idx]
            target_token -= sentence_tokens_length[idx]
            sentence_flags[idx] = True
            if target_token < 0:
                break
        idx = 0
        res = []
        for s in sentences:
            tmp = [jj for ii, jj in enumerate(s) if sentence_flags[idx + ii]]
            res.append("\n".join(tmp))
            idx += len(s)
        return res

    def get_compressed_input(
            self,
            loss,
            input_ids,
            attention_mask,
            end=200,
            iterative_size=200,
            threshold=0.5,
            keep_flag=None,
            split_token_id: int = 13,
            start: int = 0,
            self_loss=None,
            self_input_ids=None,
            self_attention_mask=None,
    ):
        """

        def get_compressed_input(self, loss, input_ids, attention_mask, end=200, iterative_size=200, threshold=0.5, keep_flag=None, split_token_id: int=13, start: int=0, self_loss=None, self_input_ids=None, self_attention_mask=None):

            Gets a compressed version of the input by filtering out tokens based on loss values and a specified threshold. Only includes important tokens to reduce input length and computational needs.

            Parameters:
                self: The instance of the class containing this method.
                loss (torch.Tensor): Tensor containing loss values for each token.
                input_ids (torch.Tensor): Tensor containing the IDs of the tokens.
                attention_mask (torch.Tensor): Tensor indicating which tokens should be attended to.
                end (int, optional): The position in the input sequence up to which tokens are considered. Defaults to 200.
                iterative_size (int, optional): The size of the window used when iterating over tokens to determine which to keep. Defaults to 200.
                threshold (float, optional): The threshold above which tokens are considered important based on loss values. Defaults to 0.5.
                keep_flag (torch.Tensor, optional): Tensor indicating whether certain tokens should always be kept regardless of loss values. Defaults to None.
                split_token_id (int, optional): The token ID used to identify split points in the sequence. Defaults to 13.
                start (int, optional): The index to start from when evaluating the loss tensor. Defaults to 0.
                self_loss (torch.Tensor, optional): An optional tensor containing self loss values for adjusting the importance of tokens. Can be used in iterative compression scenarios. Defaults to None.
                self_input_ids (torch.Tensor, optional): Tensor containing the input IDs for the self attention context. Required if 'self_loss' is provided. Defaults to None.
                self_attention_mask (torch.Tensor, optional): Tensor indicating which tokens in the self context should be attended to. Required if 'self_loss' is provided. Defaults to None.

            Returns:
                tuple: A tuple containing the compressed versions of the input_ids and attention_mask, along with possibly updated versions of keep_flag, end position, loss, self_loss, self_compressed_input_ids, and self_compressed_attention_mask.

            Raises:
                ValueError: If self_loss is provided but self_input_ids or self_attention_mask is None.
        """
        if self_loss is not None:
            need_idx = torch.concat(
                [
                    loss[:start] > 0,
                    self_loss[: loss[start:].shape[0]] - loss[start:] > threshold,
                    loss[:1] > 0,
                ]
            )
        else:
            need_idx = torch.concat([loss > threshold, loss[:1] > 0])
        need_idx[end:] = 1
        need_idx[: end - iterative_size] = 1
        loss = loss[need_idx[:-1]]
        if self_loss is not None:
            if need_idx.shape[0] < self_loss.shape[0] + start + 1:
                need_idx = torch.cat(
                    [
                        need_idx,
                        torch.ones(
                            self_loss.shape[0] - need_idx.shape[0] + start + 1,
                            dtype=torch.bool,
                        ).to(need_idx.device),
                    ]
                )
            self_loss = self_loss[need_idx[start:-1]]

        if need_idx.shape[0] < input_ids.shape[1]:
            need_idx = torch.cat(
                [
                    need_idx,
                    torch.ones(
                        input_ids.shape[1] - need_idx.shape[0], dtype=torch.bool
                    ).to(need_idx.device),
                ]
            )
        elif need_idx.shape[0] > input_ids.shape[1]:
            need_idx = need_idx[: input_ids.shape[1]]

        if keep_flag is not None:
            need_idx[keep_flag == 1] = 1
        last = -1
        if keep_flag is not None:
            for ii in range(end - iterative_size, end):
                if need_idx[ii] != 1:
                    continue
                now = input_ids[0][ii].detach().cpu().item()
                if (
                        now == split_token_id
                        and last == split_token_id
                        and keep_flag[ii].detach().cpu().item() == 0
                ):
                    need_idx[ii] = 0
                else:
                    last = now
        compressed_input_ids = input_ids[attention_mask == 1][need_idx].unsqueeze(0)
        compressed_attention_mask = attention_mask[attention_mask == 1][
            need_idx
        ].unsqueeze(0)

        if self_loss is not None:
            self_compressed_input_ids = self_input_ids[self_attention_mask == 1][
                need_idx[start:]
            ].unsqueeze(0)
            self_compressed_attention_mask = self_attention_mask[
                self_attention_mask == 1
                ][need_idx[start:]].unsqueeze(0)
        else:
            self_compressed_input_ids, self_compressed_attention_mask = None, None
        if keep_flag is not None:
            if len(keep_flag) > len(need_idx):
                keep_flag = torch.cat(
                    [
                        keep_flag[:start],
                        keep_flag[start: len(need_idx) + start][need_idx],
                        keep_flag[start + len(need_idx):],
                    ]
                )
            else:
                keep_flag = keep_flag[need_idx]
        end -= (need_idx[:end] == 0).sum()
        return (
            compressed_input_ids,
            compressed_attention_mask,
            keep_flag,
            end,
            loss,
            self_loss,
            self_compressed_input_ids,
            self_compressed_attention_mask,
        )

    def get_estimate_threshold_base_distribution(
            self, ppl, ratio: float, condition_flag: bool = False
    ):
        """

        def get_estimate_threshold_base_distribution(self, ppl, ratio: float, condition_flag: bool=False):
        """
        ppl = ppl[ppl != 10000]
        target_token = max(0, min(len(ppl) - 1, int(len(ppl) * ratio) - 1))
        return (
            ppl.sort(descending=not condition_flag)
            .values[target_token]
            .detach()
            .cpu()
            .item()
        )

    def iterative_compress_prompt(
            self,
            context: List[str],
            target_token: float,
            iterative_size: int = 200,
            keep_split: bool = False,
            split_token_id: int = 13,
            start: int = 0,
            dynamic_ratio: list = None,
            condition_compare: bool = False,
    ):
        """

        def iterative_compress_prompt(self, context: List[str], target_token: float, iterative_size: int=200, keep_split: bool=False, split_token_id: int=13, start: int=0, dynamic_ratio: list=None, condition_compare: bool=False):
        """
        iterative_ratios = self.get_dynamic_compression_ratio(
            context, target_token, iterative_size, dynamic_ratio, start
        )
        context = "\n\n".join(context)
        tokenized_text = self.tokenizer(context, return_tensors="pt")
        input_ids = tokenized_text["input_ids"].to(self.device)
        attention_mask = tokenized_text["attention_mask"].to(self.device)

        N = (attention_mask == 1).sum()
        compressed_input_ids, compressed_attention_mask = input_ids, attention_mask
        if condition_compare:
            self_input_ids, self_attention_mask = (
                input_ids[:, start:],
                attention_mask[:, start:],
            )
            self_compressed_input_ids, self_compressed_attention_mask = (
                self_input_ids,
                self_attention_mask,
            )

        end = min(iterative_size + start, compressed_input_ids.shape[1])
        threshold, keep_flag = None, None
        if keep_split:
            input_ids_numpy = input_ids.cpu().detach().numpy()[0]
            N = len(input_ids_numpy)
            keep_flag = [
                int(
                    (
                            ii > 0
                            and input_ids_numpy[ii] == split_token_id
                            and input_ids_numpy[ii - 1] == split_token_id
                    )
                    or (
                            ii < N - 1
                            and input_ids_numpy[ii] == split_token_id
                            and input_ids_numpy[ii + 1] == split_token_id
                    )
                )
                for ii in range(N)
            ]
            keep_flag = torch.tensor(keep_flag).to(self.device)
        past_key_values, past_loss, ready_end = None, None, 0
        self_past_key_values, self_past_loss, self_ready_end = None, None, 0
        pop_compressed_input_ids, pop_self_compressed_input_ids = None, None
        idx = 0
        while end <= compressed_input_ids.shape[1]:
            if end > self.max_position_embeddings and past_key_values is not None:
                # KV-Cache Compression
                e, s = end - self.max_position_embeddings, self.cache_bos_num
                if pop_compressed_input_ids is None:
                    pop_compressed_input_ids = compressed_input_ids[:, :e]
                else:
                    pop_compressed_input_ids = torch.cat(
                        [pop_compressed_input_ids, compressed_input_ids[:, :e]], dim=-1
                    )
                compressed_input_ids = compressed_input_ids[:, e:]
                compressed_attention_mask = compressed_attention_mask[:, e:]
                past_key_values = [
                    [
                        torch.cat([k[..., :s, :], k[..., s + e:, :]], dim=-2),
                        torch.cat([v[..., :s, :], v[..., s + e:, :]], dim=-2),
                    ]
                    for k, v in past_key_values
                ]
                end, ready_end = end - e, ready_end - e
                if condition_compare:
                    s = min(s, self_past_key_values[0][0].shape[2] - e)
                    self_ready_end -= e
                    if pop_self_compressed_input_ids is None:
                        pop_self_compressed_input_ids = self_compressed_input_ids[:, :e]
                    else:
                        pop_self_compressed_input_ids = torch.cat(
                            [
                                pop_self_compressed_input_ids,
                                self_compressed_input_ids[:, :e],
                            ],
                            dim=-1,
                        )
                    self_compressed_input_ids = self_compressed_input_ids[:, e:]
                    self_compressed_attention_mask = self_compressed_attention_mask[
                                                     :, e:
                                                     ]
                    self_past_key_values = [
                        [
                            torch.cat([k[..., :s, :], k[..., s + e:, :]], dim=-2),
                            torch.cat([v[..., :s, :], v[..., s + e:, :]], dim=-2),
                        ]
                        for k, v in self_past_key_values
                    ]

            loss, past_key_values = self.get_ppl(
                "",
                "token",
                compressed_input_ids,
                compressed_attention_mask,
                past_key_values=past_key_values,
                return_kv=True,
                end=end if idx else None,
            )
            if past_loss is not None:
                if end - 1 > len(past_loss):
                    past_loss = torch.cat(
                        [past_loss, torch.zeros_like(loss)[: end - 1 - len(past_loss)]]
                    )
                past_loss[ready_end: end - 1] = loss
                loss = past_loss
            else:
                past_loss = loss
            if idx:
                past_key_values = [
                    [k[:, :, : end - iterative_size], v[:, :, : end - iterative_size]]
                    for k, v in past_key_values
                ]
            else:
                past_key_values = None

            if condition_compare:
                self_loss, self_past_key_values = self.get_ppl(
                    "",
                    "token",
                    self_compressed_input_ids,
                    self_compressed_attention_mask,
                    past_key_values=self_past_key_values,
                    return_kv=True,
                    end=end - start if idx else None,
                )
                if self_past_loss is not None:
                    if end - start - 1 > len(self_past_loss):
                        self_past_loss = torch.cat(
                            [
                                self_past_loss,
                                torch.zeros_like(self_loss)[
                                : end - 1 - start - len(self_past_loss)
                                ],
                            ]
                        )
                    self_past_loss[self_ready_end: end - start - 1] = self_loss
                    self_loss = self_past_loss
                else:
                    self_past_loss = self_loss
                if idx:
                    self_past_key_values = [
                        [
                            k[:, :, : end - iterative_size - start],
                            v[:, :, : end - iterative_size - start],
                        ]
                        for k, v in self_past_key_values
                    ]
                else:
                    self_past_key_values = None

                self_ready_end = (
                    end - start - iterative_size if not (start and idx == 0) else 0
                )
            ready_end = end - iterative_size if not (start and idx == 0) else 0

            for delta_end, ratio in iterative_ratios[idx]:
                loss = past_loss
                if condition_compare:
                    self_loss = self_past_loss
                    threshold = self.get_estimate_threshold_base_distribution(
                        self_loss[: loss[start:].shape[0]] - loss[start:], ratio, False
                    )
                else:
                    threshold = self.get_estimate_threshold_base_distribution(
                        loss, ratio, False
                    )

                (
                    compressed_input_ids,
                    compressed_attention_mask,
                    keep_flag,
                    end,
                    past_loss,
                    self_past_loss,
                    self_compressed_input_ids,
                    self_compressed_attention_mask,
                ) = self.get_compressed_input(
                    loss,
                    compressed_input_ids,
                    compressed_attention_mask,
                    end - iterative_size + delta_end,
                    iterative_size=delta_end,
                    threshold=threshold,
                    keep_flag=keep_flag,
                    split_token_id=split_token_id,
                    start=start,
                    self_loss=self_loss if condition_compare else None,
                    self_input_ids=self_compressed_input_ids
                    if condition_compare
                    else None,
                    self_attention_mask=self_compressed_attention_mask
                    if condition_compare
                    else None,
                )
                end += iterative_size
            idx += 1
        if pop_compressed_input_ids is not None:
            compressed_input_ids = torch.cat(
                [pop_compressed_input_ids, compressed_input_ids], dim=-1
            )
        return compressed_input_ids[:, start:], compressed_attention_mask[:, start:]

    def recover(
            self,
            original_prompt: str,
            compressed_prompt: str,
            response: str,
    ):
        """

        def recover(self, original_prompt: str, compressed_prompt: str, response: str) -> str:
        """

        def match_from_compressed(response_word):
            """


                Calculates the best match substring from the compressed token representation of a response word.

                This function uses tokenized input ids for the response word and searches for the best matching sequence within the original token stream represented by the `original_input_ids`. The match is scored based on the number of consecutive tokens found and the compactness of the matching sequence. The best match is determined either by a higher score of consecutive token matches or by a shorter length of the sequence in case of a tie in the score. The purpose of this function is to locate a matching tokenized substring within a larger token stream, possibly for applications such as information retrieval or text alignment.

                Parameters:
                    response_word (str): The word or phrase that needs to be matched within the token stream.

                Returns:
                    str: The best matching substring decoded from `original_input_ids`, or the original `response_word` if no match is found.

                Raises:
                    This function relies on external state (`self.tokenizer`, `original_input_ids`, and `M`) and may raise various exceptions related to index errors or tokenization errors if these states are not appropriately initialized or managed.

            """
            response_input_ids = self.tokenizer(
                response_word, add_special_tokens=False
            )["input_ids"]
            response_set, response_c = set(response_input_ids), defaultdict(list)
            for idx in range(M):
                if original_input_ids[idx] in response_set:
                    response_c[original_input_ids[idx]].append(idx)
            res, res_min, res_c = None, float("inf"), 1
            n = len(response_input_ids)
            for l in response_c[response_input_ids[0]]:
                x, y, c = 0, l, 1
                for x in range(1, n):
                    idx = bisect.bisect_right(response_c[response_input_ids[x]], y)
                    if (
                            idx >= len(response_c[response_input_ids[x]])
                            or response_c[response_input_ids[x]][idx] - y > 10
                    ):
                        continue
                    c += 1
                    y = response_c[response_input_ids[x]][idx]
                if c > res_c:
                    res_c = c
                    res_min = y - l + 1
                    res = (l, y + 1)
                elif c == res_c and y - l + 1 < res_min:
                    res_min = y - l + 1
                    res = (l, y + 1)

            if res is None:
                return response_word
            # while l > 0 and not self.tokenizer.convert_ids_to_tokens(original_input_ids[l]).startswith("_"):
            #     l -= 1
            # while r < M - 1 and not self.tokenizer.convert_ids_to_tokens(original_input_ids[l]).startswith("_"):
            #     l -= 1
            return self.tokenizer.decode(original_input_ids[res[0]: res[1]])

        response_words = response.split(" ")

        original_input_ids = self.tokenizer(original_prompt, add_special_tokens=False)[
            "input_ids"
        ]
        N, M = len(response_words), len(original_input_ids)
        recovered_response_words = []
        l = 0
        while l < N:
            if response_words[l] not in compressed_prompt:
                recovered_response_words.append(response_words[l])
                l += 1
                continue
            r = l
            while (
                    r + 1 < N and " ".join(response_words[l: r + 2]) in compressed_prompt
            ):
                r += 1

            match_words = match_from_compressed(" ".join(response_words[l: r + 1]))
            recovered_response_words.append(match_words)
            l = r + 1
        return " ".join(recovered_response_words)

    def get_rank_results(
            self,
            context: list,
            question: str,
            rank_method: str,
            condition_in_question: str,
            context_tokens_length: list,
    ):
        """

        def get_rank_results(self, context: list, question: str, rank_method: str, condition_in_question: str, context_tokens_length: list):
            "/**
            Computes a ranked list of documents from the given context that are most relevant to a specified question using diverse ranking methods.

            This function offers a flexible querying interface that can integrate with various text ranking mechanisms such as BM25, gzip-based similarity, sentence-BERT models, OpenAI's embedding models, among others. The final ranking is influenced by additional factors like question conditions and context token lengths, allowing for bespoke adjustments based on the method chosen.

            Args:
                context (list): A list of documents (strings) among which the relevant ones are to be found.
                question (str): The query or question that the documents are checked against for relevance.
                rank_method (str): A string representing the ranking method to be used. It should match one of the predefined method names.
                condition_in_question (str): A string that represents additional conditions considered in questioning to adjust the ranking.
                context_tokens_length (list): A list of integers representing the lengths of tokens in each of the documents in the context.

            Returns:
                list: A list of tuples, where each tuple contains the index of the document in the original context list and an associated score or sorting criteria. For most methods, the score is not meaningful and the ranking is implicit in the order of indices.

            Raises:
                ValueError: An error occurs if the rank_method specified is not amongst the supported methods.

            Note:
                Users must ensure that the necessary third-party libraries (like rank_bm25, sentence_transformers, gzip, openai, transformers, voyageai, cohere) are installed and configured properly for some of the ranking methods to work.
            */
        }
        """

        def get_distance_bm25(corpus, query):
            """

            Calculate the BM25 distance between the query and each document in the corpus.

            This function computes similarity scores using the BM25 ranking function
            applied to a tokenized corpus of documents and a tokenized query. The
            BM25 ranking function considers term frequency (TF) and inverse document
            frequency (IDF), as well as document length normalization, making it
            a popular and robust method for scoring documents in information retrieval.
            The higher the BM25 score, the more relevant a document is considered
            to the query.

            Parameters:
                corpus (List[str]): A list of document strings to be searched within.
                query (str): The query string for which the similarity scores will be
                             calculated relative to each document in the corpus.

            Returns:
                List[Tuple[int, int]]: A sorted list of tuples, each containing the
                                       index of a document in the corpus and a placeholder
                                       zero. The list is sorted in descending order of
                                       BM25 scores, meaning that documents are ranked from
                                       most to least relevant to the query.

            Raises:
                ImportError: If the 'rank_bm25' package is not installed.

            Note:
                The placeholder zero in the returned tuples does not carry any meaningful
                information in the current implementation and is merely used to conform to
                the output format expected by the system consuming this function.

            The 'rank_bm25' package must be installed to use this function. To install,
            run `pip install rank_bm25`.

            """
            from rank_bm25 import BM25Okapi

            tokenized_corpus = [doc.split(" ") for doc in corpus]
            bm25 = BM25Okapi(tokenized_corpus)
            tokenized_query = query.split(" ")
            doc_scores = bm25.get_scores(tokenized_query)
            idx = [(ii, 0) for ii in (-doc_scores).argsort()]
            return idx

        def get_distance_gzip(corpus, query):
            """

            Calculate the normalized compression distance between multiple documents and a query string using gzip compression.

            This function utilizes the concept of the normalized compression distance to determine the similarity between a corpus of documents and a given query string. It encodes and compresses the documents and the query, and computes a score reflecting their relative compressibility when concatenated. Lower scores correspond to higher similarity.

            The function returns a list of tuples, with each tuple containing the index of a document in the corpus and its associated relative score. The list is sorted by score in ascending order, so documents more similar to the query will appear first.

            Args:
                corpus (list of str): A list of document strings to compare against the query.
                query (str): The query string to compare with the documents in the corpus.

            Returns:
                list of tuple: A sorted list of tuples where each tuple contains an index of the document in the original corpus and its associated score. The sorting is based on the scores in ascending order.
            """

            def get_score(x, y):
                """

                def get_score(x, y):
                """
                cx, cy = len(gzip.compress(x.encode())), len(gzip.compress(y.encode()))
                cxy = len(gzip.compress(f"{x} {y}".encode()))
                return (cxy - min(cx, cy)) / max(cx, cy)

            import gzip

            doc_scores = [get_score(doc, query) for doc in corpus]
            idx = [(ii, 0) for ii in np.argsort(doc_scores)]
            return idx

        def get_distance_sentbert(corpus, query):
            """

            def get_distance_sentbert(corpus, query):
            """
            from sentence_transformers import SentenceTransformer, util

            if self.retrieval_model is None or self.retrieval_model_name != rank_method:
                self.retrieval_model = SentenceTransformer("multi-qa-mpnet-base-dot-v1")
                self.retrieval_model_name = rank_method
            doc_embeds = self.retrieval_model.encode(corpus)
            query = self.retrieval_model.encode(query)
            doc_scores = -util.dot_score(doc_embeds, query).cpu().numpy().reshape(-1)
            idx = [(ii, 0) for ii in np.argsort(doc_scores)]
            return idx

        def get_distance_openai(corpus, query):
            """

            def get_distance_openai(corpus, query):
            """
            import openai
            from sentence_transformers import util

            openai.api_key = self.open_api_config.get("api_key", "")
            openai.api_base = self.open_api_config.get(
                "api_base", "https://api.openai.com/v1"
            )
            openai.api_type = self.open_api_config.get("api_type", "open_ai")
            openai.api_version = self.open_api_config.get("api_version", "2023-05-15")
            engine = self.open_api_config.get("engine", "text-embedding-ada-002")

            def get_embed(text):
                """


                    Retrieves the embedding representation of a given text using the OpenAI API.

                    This function takes a string of text, preprocesses it by replacing newline characters with spaces, and then uses the OpenAI API's
                    Embedding endpoint to generate an embedding for the text. It assumes that there is an OpenAI client (`openai`) already
                    instantiated with the appropriate API key and that the `engine` variable is defined globally or in the calling scope to specify
                    the engine to be used for creating embeddings.

                    Args:
                        text (str): The text for which to generate the embedding. Newline characters in the text will be replaced with spaces.

                    Returns:
                        list: The embedding of the given text as a list of floats. This is extracted from the API's response.

                    Raises:
                        openai.error.OpenAIError: If the OpenAI API call fails for reasons such as authentication issues or invalid inputs.

                """
                return openai.Embedding.create(
                    input=[text.replace("\n", " ")], engine=engine
                )["data"][0]["embedding"]

            doc_embeds = [get_embed(i) for i in corpus]
            query = get_embed(query)
            doc_scores = -util.dot_score(doc_embeds, query).cpu().numpy().reshape(-1)
            idx = [(ii, 0) for ii in np.argsort(doc_scores)]
            return idx

        def get_distance_sentbert_bge(corpus, query):
            """

            def get_distance_sentbert_bge(corpus, query):
            """
            from sentence_transformers import SentenceTransformer, util

            if self.retrieval_model is None or self.retrieval_model_name != rank_method:
                self.retrieval_model = SentenceTransformer("BAAI/bge-large-en-v1.5")
                self.retrieval_model_name = rank_method
            doc_embeds = self.retrieval_model.encode(
                [i for i in corpus], normalize_embeddings=True
            )
            query = self.retrieval_model.encode(query, normalize_embeddings=True)
            doc_scores = -util.dot_score(doc_embeds, query).cpu().numpy().reshape(-1)
            idx = [(ii, 0) for ii in np.argsort(doc_scores)]
            return idx

        def get_distance_bge_ranker(corpus, query):
            """

            def get_distance_bge_ranker(corpus, query):

                Computes the relevance scores of documents in the corpus to the given query using the 'BAAI/bge-reranker-large' transformer model.

                The function takes a list of text documents and a query string, and it returns a sorted list of tuples where each tuple contains the index of the document in the original corpus and the associated score which is set to 0.

                The ranking is determined by the relevance of the corpus documents to the query as scored by the transformer model. Higher relevance results in a lower index in the sorted output.

                Parameters:
                corpus (list of str): A list of text documents to be ranked.
                query (str): The query string that will be used to rank the documents in the corpus.

                Returns:
                list of (int, float): A sorted list of tuples, where each tuple contains the index of the document in the original corpus and the score set to 0, sorted by descending relevance to the query.

                Note:
                - The retrieval model is loaded from 'BAAI/bge-reranker-large' if it is not already loaded or if the 'rank_method' variable has changed.
                - The model and tokenizer are loaded using the transformers library `AutoModelForSequenceClassification` and `AutoTokenizer` classes.
                - The documents and the query are tokenized, scored, and sorted based on relevance within a no-grad context to avoid tracking gradients, which is unnecessary during the inference.
                - The outputs are moved to the CPU, and scores are converted to a float for consistent processing.
                - The method assumes the use of a PyTorch device object stored in self.device where the model is run.

            """
            from transformers import AutoModelForSequenceClassification, AutoTokenizer

            pairs = [[i, query] for i in corpus]
            if self.retrieval_model is None or self.retrieval_model_name != rank_method:
                tokenizer = AutoTokenizer.from_pretrained("BAAI/bge-reranker-large")
                model = (
                    AutoModelForSequenceClassification.from_pretrained(
                        "BAAI/bge-reranker-large"
                    )
                    .eval()
                    .to(self.device)
                )
                self.retrieval_model = [tokenizer, model]
                self.retrieval_model_name = rank_method
            with torch.no_grad():
                inputs = self.retrieval_model[0](
                    pairs,
                    padding=True,
                    truncation=True,
                    return_tensors="pt",
                    max_length=512,
                ).to(self.device)
                scores = (
                    self.retrieval_model[1](**inputs, return_dict=True)
                    .logits.view(
                        -1,
                    )
                    .float()
                )
            idx = [(ii, 0) for ii in np.argsort(-scores.cpu())]
            return idx

        def get_distance_bge_llmembedder(corpus, query):
            """

            get_distance_bge_llmembedder(corpus, query)

            Computes the distance between a query and a corpus of documents using embeddings from the Language Model Embedder.

            This function initializes and utilizes a language model embedder (specifically the 'BAAI/llm-embedder' from Hugging Face's transformer library) to encode both the query and each document in the corpus, and then calculates the cosine similarity between the query and document embeddings. The similarity scores are used to perform document retrieval, ranking each document in the corpus relative to how semantically similar it is to the given query.

            Args:
                corpus (list of str): A list of documents, each represented as a string.
                query (str): The query string used to find relevant documents within the corpus.

            Returns:
                list of tuple: A sorted list of tuples, where each tuple contains an index of the document in the corpus and a dummy value (0), sorted in descending order of similarity to the query.

            Note:
                The function assumes that it is being called within an instance of a class that contains 'self.retrieval_model' and 'self.retrieval_model_name', as well as 'self.device'. It updates or initializes 'self.retrieval_model' and 'self.retrieval_model_name' based on the 'rank_method' (which is not provided as an argument to this function and is assumed to be a part of the class).

                The embeddings are obtained using the '[CLS]' token representation from the last hidden layer of the language model. Before computing the cosine similarity, embeddings are normalized using the L2 norm. The cosine similarity is computed as the dot product of the normalized query and document embeddings. The function does not persist these embeddings and does not modify the original corpus.

                This function requires the 'transformers' library to be installed and a compatible device (e.g., a GPU) to be available for the model to run on.

                This function operates in a no-gradient context to avoid computing gradients and to work efficiently in evaluation mode.
            """
            from transformers import AutoModel, AutoTokenizer

            if self.retrieval_model is None or self.retrieval_model_name != rank_method:
                tokenizer = AutoTokenizer.from_pretrained("BAAI/llm-embedder")
                model = (
                    AutoModel.from_pretrained("BAAI/llm-embedder")
                    .eval()
                    .to(self.device)
                )
                self.retrieval_model = [tokenizer, model]
                self.retrieval_model_name = rank_method

            instruction_qa_query = (
                "Represent this query for retrieving relevant documents: "
            )
            instruction_qa_key = "Represent this document for retrieval: "
            queries = [instruction_qa_query + query for _ in corpus]
            keys = [instruction_qa_key + key for key in corpus]
            with torch.no_grad():
                query_inputs = self.retrieval_model[0](
                    queries,
                    padding=True,
                    truncation=True,
                    return_tensors="pt",
                    max_length=512,
                ).to(self.device)
                key_inputs = self.retrieval_model[0](
                    keys,
                    padding=True,
                    truncation=True,
                    return_tensors="pt",
                    max_length=512,
                ).to(self.device)
                query_outputs = self.retrieval_model[1](**query_inputs)
                key_outputs = self.retrieval_model[1](**key_inputs)
                # CLS pooling
                query_embeddings = query_outputs.last_hidden_state[:, 0]
                key_embeddings = key_outputs.last_hidden_state[:, 0]
                # Normalize
                query_embeddings = torch.nn.functional.normalize(
                    query_embeddings, p=2, dim=1
                )
                key_embeddings = torch.nn.functional.normalize(
                    key_embeddings, p=2, dim=1
                )
                similarity = query_embeddings @ key_embeddings.T
            idx = [(ii, 0) for ii in np.argsort(-similarity[0].cpu())]
            return idx

        def get_distance_jinza(corpus, query):
            """


                Calculates the distance between a corpus of documents and a query string using a pre-trained embedding model from Jina AI.

                This function encodes both the corpus and the query string into vector embeddings using the 'jinaai/jina-embeddings-v2-base-en' model. It then calculates the cosine similarity between the query vector and each of the document vectors. The function sorts the documents based on their similarity to the query in descending order and returns a list of tuples, each consisting of the document index and a placeholder value 0.

                Parameters:
                    corpus (list of str): A list of documents as strings.
                    query (str): The query string for which the similarity against the corpus is to be computed.

                Returns:
                    list of tuple: Each tuple contains an index of the document in the 'corpus' list and a placeholder similarity score of 0. The list is sorted by the similarity of each document to the query, in descending order.

                Note:
                    - The model and tokenizer are assumed to be compatible with the corpus and query provided.
                    - The function assumes that 'self.retrieval_model' and 'self.retrieval_model_name' are attributes of the class instance which 'get_distance_jinza' is a part of. The function updates these attributes accordingly.
                    - The 'AutoModel.from_pretrained' method is used to load the pre-trained model, and 'trust_remote_code=True' indicates that the code associated with the remote model can be trusted and executed.
                    - The function uses numpy for calculations and requires the HuggingFace 'transformers' library for the model.
                    - The actual similarity score is not returned, only the index of the documents in sorted order is returned with a placeholder similarity score of 0.

            """
            from numpy.linalg import norm

            from transformers import AutoModel

            def cos_sim(a, b):
                """


                    Calculate the cosine similarity between two vectors.

                    Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them.
                    The cosine similarity is particularly used in positive space, where the outcome is neatly bounded in [0,1].

                    Parameters:
                    a : array
                        A numpy array representing the first vector.

                    b : array
                        A numpy array representing the second vector.

                    Returns:
                    float
                        A float value representing the cosine similarity between the two input vectors. If both vectors are all-zeroes, the function will return NaN, which stands for 'Not a Number'.

                    Raises:
                    ValueError
                        If the input arrays are not of compatible dimensions for dot product calculation or if they do not conform to the expected format (e.g., non-numeric types).

                """
                return (a @ b.T) / (norm(a) * norm(b))

            if self.retrieval_model is None or self.retrieval_model_name != rank_method:
                model = (
                    AutoModel.from_pretrained(
                        "jinaai/jina-embeddings-v2-base-en", trust_remote_code=True
                    )
                    .eval()
                    .to(self.device)
                )
                self.retrieval_model = model
                self.retrieval_model_name = rank_method

            doc_embeds = self.retrieval_model.encode(corpus)
            query = self.retrieval_model.encode(query)
            doc_scores = cos_sim(doc_embeds, query)
            idx = [(ii, 0) for ii in np.argsort(-doc_scores)]
            return idx

        def get_distance_voyageai(corpus, query):
            """

            def get_distance_voyageai(corpus, query):
            """
            import voyageai
            from sentence_transformers import util

            voyageai.api_key = self.open_api_config.get("voyageai_api_key", "")

            def get_embed(text):
                """

                def get_embed(text):
                """
                return voyageai.get_embedding(text, model="voyage-01")

            doc_embeds = [get_embed(i) for i in corpus]
            query = get_embed(query)
            doc_scores = -util.dot_score(doc_embeds, query).cpu().numpy().reshape(-1)
            idx = [(ii, 0) for ii in np.argsort(doc_scores)]
            return idx

        def get_distance_cohere(corpus, query):
            """

            def get_distance_cohere(corpus, query):
            """
            import cohere

            api_key = self.open_api_config.get("cohere_api_key", "")
            co = cohere.Client(api_key)
            results = co.rerank(
                model="rerank-english-v2.0", query=query, documents=corpus, top_n=20
            )
            c_map = {jj: ii for ii, jj in enumerate(corpus)}
            doc_rank = [c_map[ii.document["text"]] for ii in results]
            idx = [(ii, 0) for ii in doc_rank]
            return idx

        def get_distance_longllmlingua(corpus, query):
            """

            Calculates a relevance score for each document in a corpus related to a given query under a certain condition.

            This function computes a score for each document in the provided corpus based on how likely the document's language matches the given query under a specified condition. If the condition is 'none', the results are sorted in descending order; otherwise, they are sorted in ascending order. It does not take the length of context into consideration as the related term in the formula is multiplied by zero.

            Parameters:
                corpus (list): A list of documents against which the query is to be evaluated.
                query (str): The query string for which relevance is to be determined.
                condition_in_question (str): The specified condition that affects the sorting direction.
                context_tokens_length (list): A list of token lengths corresponding to each document.

            Returns:
                list of tuples: A sorted list where each tuple contains the document index and the computed score.

            Raises:
                NameError: If 'condition_in_question' or 'context_tokens_length' are not passed as arguments despite being used in the method.
            """
            context_ppl = [
                self.get_condition_ppl(
                    d,
                    query
                    + " We can get the answer to this question in the given documents.",
                    condition_in_question,
                )
                - dl * 2 / 250 * 0
                for d, dl in zip(corpus, context_tokens_length)
            ]
            sort_direct = -1 if condition_in_question == "none" else 1
            ys = sorted(enumerate(context_ppl), key=lambda x: sort_direct * x[1])
            return ys

        method = None
        if rank_method == "bm25":
            method = get_distance_bm25
        elif rank_method == "gzip":
            method = get_distance_gzip
        elif rank_method == "sentbert":
            method = get_distance_sentbert
        elif rank_method == "openai":
            method = get_distance_openai
        elif rank_method in ["longllmlingua", "llmlingua"]:
            method = get_distance_longllmlingua
        elif rank_method == "bge":
            method = get_distance_sentbert_bge
        elif rank_method == "bge_reranker":
            method = get_distance_bge_ranker
        elif rank_method == "bge_llmembedder":
            method = get_distance_bge_llmembedder
        elif rank_method == "jinza":
            method = get_distance_jinza
        elif rank_method == "voyageai":
            method = get_distance_voyageai
        elif rank_method == "cohere":
            method = get_distance_cohere
        return method(context, question)
